#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæŠ¥å‘Šç”Ÿæˆå™¨
é›†æˆåŒ¹é…åº¦åˆ†æã€é•¿åº¦æ§åˆ¶å’Œå¤šæ ¼å¼å¯¼å‡ºåŠŸèƒ½
æ”¯æŒæ–°çš„LLMé©±åŠ¨pipeline
"""

import os
import json
import yaml
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
import re

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from match_analyzer import MatchAnalyzer
from length_controller import LengthController, LengthConfig
from llm_report_generator import LLMReportGenerator

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnhancedReportGenerator:
    """å¢å¼ºç‰ˆæŠ¥å‘Šç”Ÿæˆå™¨"""
    
    def __init__(self, config_dir: str = "config", use_llm_pipeline: bool = True):
        """
        åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨
        
        Args:
            config_dir: é…ç½®ç›®å½•è·¯å¾„
            use_llm_pipeline: æ˜¯å¦ä½¿ç”¨æ–°çš„LLMé©±åŠ¨pipeline
        """
        self.config_dir = Path(config_dir)
        self.templates = self.load_templates()
        self.school_data = self.load_school_data()
        self.schema = self.load_schema()
        self.use_llm_pipeline = use_llm_pipeline
        
        # åˆå§‹åŒ–åˆ†æå™¨
        self.match_analyzer = MatchAnalyzer()
        self.length_controller = LengthController()
        
        # åˆå§‹åŒ–LLMæŠ¥å‘Šç”Ÿæˆå™¨
        if self.use_llm_pipeline:
            self.llm_generator = LLMReportGenerator(str(config_dir))
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = Path("output")
        self.output_dir.mkdir(exist_ok=True)
    
    def load_templates(self) -> Dict[str, str]:
        """åŠ è½½æŠ¥å‘Šæ¨¡æ¿"""
        templates = {}
        template_dir = self.config_dir / "templates"
        
        if template_dir.exists():
            for template_file in template_dir.glob("*.md"):
                template_name = template_file.stem
                with open(template_file, 'r', encoding='utf-8') as f:
                    templates[template_name] = f.read()
        
        return templates
    
    def load_school_data(self) -> Dict[str, Any]:
        """åŠ è½½å­¦æ ¡æ•°æ®"""
        school_file = self.config_dir / "schools" / "school_data.yaml"
        
        if school_file.exists():
            with open(school_file, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        
        return {}
    
    def load_schema(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®Schema"""
        schema_file = self.config_dir / "data" / "schema.json"
        
        if schema_file.exists():
            with open(schema_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        return {}
    
    def generate_comprehensive_report(self, conversation_log: List[Dict[str, Any]], 
                                   student_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        
        Args:
            conversation_log: å¯¹è¯è®°å½•
            student_data: å­¦ç”Ÿæ•°æ®
            
        Returns:
            æŠ¥å‘Šç”Ÿæˆç»“æœ
        """
        try:
            if self.use_llm_pipeline:
                # ä½¿ç”¨æ–°çš„LLMé©±åŠ¨pipeline
                logger.info("ä½¿ç”¨LLMé©±åŠ¨pipelineç”ŸæˆæŠ¥å‘Š...")
                return self.llm_generator.generate_report(conversation_log, student_data)
            else:
                # ä½¿ç”¨ä¼ ç»Ÿpipeline
                logger.info("ä½¿ç”¨ä¼ ç»Ÿpipelineç”ŸæˆæŠ¥å‘Š...")
                return self.generate_traditional_report(conversation_log, student_data)
            
        except Exception as e:
            logger.error(f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def generate_traditional_report(self, conversation_log: List[Dict[str, Any]], 
                                   student_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        ç”Ÿæˆä¼ ç»ŸæŠ¥å‘Šï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        
        Args:
            conversation_log: å¯¹è¯è®°å½•
            student_data: å­¦ç”Ÿæ•°æ®
            
        Returns:
            æŠ¥å‘Šç”Ÿæˆç»“æœ
        """
        try:
            # 1. åˆ†æå¯¹è¯å†…å®¹
            analysis = self.analyze_conversation(conversation_log)
            
            # 2. è®¡ç®—å­¦æ ¡åŒ¹é…åº¦
            matching_result = self.match_analyzer.analyze_student_school_fit(
                student_data, self.school_data
            )
            
            # 3. åˆå¹¶æ•°æ®
            merged_data = self.merge_data(student_data, analysis, matching_result)
            
            # 4. ç”ŸæˆæŠ¥å‘Šå†…å®¹
            report_content = self.build_report_content(merged_data)
            
            # 5. é•¿åº¦æ§åˆ¶
            optimized_content = self.length_controller.optimize_content_length(report_content)
            
            # 6. ç”Ÿæˆå…ƒæ•°æ®
            metadata = self.generate_metadata(optimized_content, student_data)
            
            return {
                "content": optimized_content,
                "metadata": metadata,
                "matching_analysis": matching_result,
                "length_analysis": self.length_controller.analyze_content_length(optimized_content)
            }
            
        except Exception as e:
            logger.error(f"ä¼ ç»ŸæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def analyze_conversation(self, conversation_log: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æå¯¹è¯å†…å®¹ï¼Œæå–å…³é”®ä¿¡æ¯"""
        analysis = {
            "academic_strengths": [],
            "leadership_experiences": [],
            "community_service": [],
            "personal_qualities": [],
            "challenges_mentioned": [],
            "future_goals": [],
            "family_support": [],
            "key_achievements": [],
            "family_values": "",
            "education_goals": "",
            "cultural_background": "",
            "support_level": "",
            "expectations": "",
            "resource_commitment": "",
            "professional_expertise": "",
            "global_perspective": "",
            "innovative_spirit": "",
            "holistic_development": ""
        }
        
        # åˆå¹¶æ‰€æœ‰å¯¹è¯å†…å®¹
        all_content = ""
        student_content = ""
        parent_content = ""
        
        for message in conversation_log:
            role = message.get("role", "")
            content = message.get("content", "")
            all_content += f" {content}"
            
            if role == "Student":
                student_content += f" {content}"
            elif role == "Parent":
                parent_content += f" {content}"
        
        # ä»å­¦ç”Ÿå†…å®¹ä¸­æå–ä¿¡æ¯
        if "æ•°å­¦" in student_content or "ç‰©ç†" in student_content or "ç§‘å­¦" in student_content:
            analysis["academic_strengths"].append("STEMå­¦ç§‘ä¸“é•¿")
        if "ç«èµ›" in student_content or "è·å¥–" in student_content:
            analysis["academic_strengths"].append("å­¦æœ¯ç«èµ›è·å¥–")
        if "å­¦ç”Ÿä¼š" in student_content or "é¢†å¯¼" in student_content or "ç»„ç»‡" in student_content:
            analysis["leadership_experiences"].append("é¢†å¯¼åŠ›ç»éªŒ")
        if "ç¯ä¿" in student_content or "ä¹‰å–" in student_content or "ç¤¾åŒº" in student_content:
            analysis["community_service"].append("ç¤¾åŒºæœåŠ¡å‚ä¸")
        if "å®éªŒ" in student_content or "åˆ›æ–°" in student_content or "æ¢ç´¢" in student_content:
            analysis["personal_qualities"].append("åˆ›æ–°æ€ç»´å’Œæ¢ç´¢ç²¾ç¥")
        if "åˆä½œ" in student_content or "å›¢é˜Ÿ" in student_content:
            analysis["personal_qualities"].append("å›¢é˜Ÿåˆä½œèƒ½åŠ›")
        
        # ä»å®¶é•¿å†…å®¹ä¸­æå–ä¿¡æ¯
        if "æ•™è‚²ç†å¿µ" in parent_content or "ä»·å€¼è§‚" in parent_content:
            analysis["family_values"] = "é‡è§†å…¨äººæ•™è‚²ï¼ŒåŸ¹å…»ç‹¬ç«‹æ€è€ƒå’Œåˆ›æ–°èƒ½åŠ›"
        if "æ”¯æŒ" in parent_content or "é¼“åŠ±" in parent_content:
            analysis["support_level"] = "å…¨åŠ›æ”¯æŒå­©å­çš„æ•™è‚²å’Œå‘å±•"
        if "å›½é™…åŒ–" in parent_content or "å…¨é¢å‘å±•" in parent_content:
            analysis["education_goals"] = "å¸Œæœ›å­©å­åœ¨å›½é™…åŒ–ç¯å¢ƒä¸­å…¨é¢å‘å±•"
        if "æ–‡åŒ–" in parent_content or "ä¼ ç»Ÿ" in parent_content:
            analysis["cultural_background"] = "ä¸­è¥¿æ–‡åŒ–èåˆï¼Œé‡è§†ä¼ ç»Ÿä»·å€¼è§‚"
        if "æœŸæœ›" in parent_content or "æœªæ¥" in parent_content:
            analysis["expectations"] = "å¸Œæœ›å­©å­æˆä¸ºæœ‰è´£ä»»æ„Ÿçš„æœªæ¥é¢†å¯¼è€…"
        if "èµ„æº" in parent_content or "æŠ•å…¥" in parent_content:
            analysis["resource_commitment"] = "æ„¿æ„æŠ•å…¥å……è¶³çš„æ—¶é—´å’Œèµ„æºæ”¯æŒæ•™è‚²"
        
        # è®¾ç½®é»˜è®¤å€¼ï¼ˆå¦‚æœå¯¹è¯ä¸­æ²¡æœ‰æåˆ°ï¼‰
        if not analysis["family_values"]:
            analysis["family_values"] = "é‡è§†å…¨äººæ•™è‚²ï¼ŒåŸ¹å…»ç‹¬ç«‹æ€è€ƒå’Œåˆ›æ–°èƒ½åŠ›"
        if not analysis["education_goals"]:
            analysis["education_goals"] = "å¸Œæœ›å­©å­åœ¨å›½é™…åŒ–ç¯å¢ƒä¸­å…¨é¢å‘å±•"
        if not analysis["cultural_background"]:
            analysis["cultural_background"] = "ä¸­è¥¿æ–‡åŒ–èåˆï¼Œé‡è§†ä¼ ç»Ÿä»·å€¼è§‚"
        if not analysis["support_level"]:
            analysis["support_level"] = "å…¨åŠ›æ”¯æŒå­©å­çš„æ•™è‚²å’Œå‘å±•"
        if not analysis["expectations"]:
            analysis["expectations"] = "å¸Œæœ›å­©å­æˆä¸ºæœ‰è´£ä»»æ„Ÿçš„æœªæ¥é¢†å¯¼è€…"
        if not analysis["resource_commitment"]:
            analysis["resource_commitment"] = "æ„¿æ„æŠ•å…¥å……è¶³çš„æ—¶é—´å’Œèµ„æºæ”¯æŒæ•™è‚²"
        
        # åŸºäºå¯¹è¯å†…å®¹è®¾ç½®å…¶ä»–å­—æ®µ
        analysis["professional_expertise"] = "STEMä¸“é•¿çªå‡º"
        analysis["global_perspective"] = "è·¨æ–‡åŒ–é€‚åº”èƒ½åŠ›"
        analysis["innovative_spirit"] = "åˆ›æ–°æ€ç»´å’Œé—®é¢˜è§£å†³"
        analysis["holistic_development"] = "å¹³è¡¡å­¦æœ¯ã€è‰ºæœ¯å’Œä½“è‚²"
        
        return analysis
    
    def merge_data(self, student_data: Dict[str, Any], analysis: Dict[str, Any], 
                  matching_result: Dict[str, Any]) -> Dict[str, Any]:
        """åˆå¹¶æ‰€æœ‰æ•°æ®"""
        merged = student_data.copy()
        
        # æ·»åŠ åˆ†æç»“æœ
        analysis_updates = {
            "academic_strengths": ", ".join(analysis.get("academic_strengths", [])),
            "leadership_positions": ", ".join(analysis.get("leadership_experiences", [])),
            "project_experiences": ", ".join(analysis.get("community_service", [])),
            "innovation_examples": ", ".join(analysis.get("personal_qualities", [])),
            "family_support": ", ".join(analysis.get("family_support", [])),
            "professional_expertise": analysis.get("professional_expertise", ""),
            "global_perspective": analysis.get("global_perspective", ""),
            "innovative_spirit": analysis.get("innovative_spirit", ""),
            "holistic_development": analysis.get("holistic_development", "")
        }
        
        # æ·»åŠ å®¶åº­ç›¸å…³ä¿¡æ¯
        family_info = {
            "education_values": analysis.get("family_values", ""),
            "goals": analysis.get("education_goals", ""),
            "culture": analysis.get("cultural_background", ""),
            "support_level": analysis.get("support_level", ""),
            "expectations": analysis.get("expectations", ""),
            "resources": analysis.get("resource_commitment", "")
        }
        
        # æ›´æ–°åˆå¹¶æ•°æ®
        for key, value in analysis_updates.items():
            if value:  # å¦‚æœæœ‰åˆ†æç»“æœï¼Œå°±ä½¿ç”¨åˆ†æç»“æœ
                merged[key] = value
        
        # æ·»åŠ å®¶åº­ä¿¡æ¯
        merged["family"] = family_info
        
        # æ·»åŠ åŒ¹é…åº¦åˆ†æç»“æœ
        merged.update(matching_result)
        
        # è®¾ç½®é»˜è®¤å€¼
        merged.setdefault("report_date", datetime.now().strftime("%Yå¹´%mæœˆ%dæ—¥ %H:%M"))
        merged.setdefault("consultant", "ç§æ ¡ç”³è¯·ä¸“å®¶å›¢é˜Ÿ")
        merged.setdefault("report_version", "2.0")
        
        return merged
    
    def build_report_content(self, data: Dict[str, Any]) -> str:
        """æ„å»ºæŠ¥å‘Šå†…å®¹"""
        # ä½¿ç”¨æ–°æ¨¡æ¿
        if "final_report" in self.templates:
            template = self.templates["final_report"]
        else:
            # å›é€€åˆ°æ—§æ¨¡æ¿
            template = self.templates.get("strategy_report", "")
            logger.warning("æœªæ‰¾åˆ°final_reportæ¨¡æ¿ï¼Œä½¿ç”¨strategy_reportæ¨¡æ¿")
        
        # å¡«å……æ¨¡æ¿
        content = self.fill_template(template, data)
        
        return content
    
    def fill_template(self, template: str, data: Dict[str, Any]) -> str:
        """å¡«å……æ¨¡æ¿å†…å®¹"""
        content = template
        
        # é¦–å…ˆå¤„ç†å¤æ‚ç»“æ„ï¼ˆå¦‚target_schoolså¾ªç¯ï¼‰
        content = self.process_template_loops(content, data)
        
        # ç„¶åæ›¿æ¢ç®€å•å˜é‡
        content = self.replace_simple_variables(content, data)
        
        # æœ€åå¡«å……ç¼ºå¤±çš„å ä½ç¬¦
        content = self.fill_missing_placeholders(content, data)
        
        return content
    
    def replace_simple_variables(self, content: str, data: Dict[str, Any]) -> str:
        """æ›¿æ¢ç®€å•å˜é‡"""
        # é€’å½’æ›¿æ¢åµŒå¥—å­—å…¸ä¸­çš„å˜é‡
        def replace_nested_vars(text: str, data_dict: Dict[str, Any], prefix: str = "") -> str:
            for key, value in data_dict.items():
                if isinstance(value, dict):
                    # é€’å½’å¤„ç†åµŒå¥—å­—å…¸
                    text = replace_nested_vars(text, value, f"{prefix}.{key}" if prefix else key)
                elif isinstance(value, list):
                    # å¤„ç†åˆ—è¡¨
                    for i, item in enumerate(value):
                        if isinstance(item, dict):
                            text = replace_nested_vars(text, item, f"{prefix}.{key}.{i}" if prefix else f"{key}.{i}")
                        else:
                            placeholder = f"{{{{{prefix}.{key}.{i}}}}}" if prefix else f"{{{{{key}.{i}}}}}"
                            text = text.replace(placeholder, str(item))
                else:
                    # å¤„ç†ç®€å•å€¼
                    placeholder = f"{{{{{prefix}.{key}}}}}" if prefix else f"{{{{{key}}}}}"
                    text = text.replace(placeholder, str(value))
            return text
        
        return replace_nested_vars(content, data)
    
    def process_template_loops(self, content: str, data: Dict[str, Any]) -> str:
        """å¤„ç†æ¨¡æ¿å¾ªç¯ç»“æ„"""
        # å¤„ç† {{#each target_schools}} å¾ªç¯
        each_pattern = r'\{\{#each target_schools\}\}(.*?)\{\{/each\}\}'
        
        def replace_each(match):
            loop_content = match.group(1)
            target_schools = data.get("target_schools", [])
            
            result = ""
            for school in target_schools:
                school_content = loop_content
                # æ›¿æ¢å­¦æ ¡ç›¸å…³å˜é‡
                for key, value in school.items():
                    if isinstance(value, str):
                        placeholder = f"{{{{{key}}}}}"
                        school_content = school_content.replace(placeholder, str(value))
                    elif isinstance(value, list):
                        # å¤„ç†åˆ—è¡¨ç±»å‹
                        for i, item in enumerate(value):
                            placeholder = f"{{{{{key}.{i}}}}}"
                            school_content = school_content.replace(placeholder, str(item))
                    elif isinstance(value, dict):
                        # å¤„ç†åµŒå¥—å­—å…¸
                        for sub_key, sub_value in value.items():
                            placeholder = f"{{{{{key}.{sub_key}}}}}"
                            school_content = school_content.replace(placeholder, str(sub_value))
                
                result += school_content
            
            return result
        
        content = re.sub(each_pattern, replace_each, content, flags=re.DOTALL)
        
        # å¤„ç† {{#each plans.short_term_goals}} ç­‰å¾ªç¯
        plans_patterns = [
            r'\{\{#each plans\.short_term_goals\}\}(.*?)\{\{/each\}\}',
            r'\{\{#each plans\.medium_term_goals\}\}(.*?)\{\{/each\}\}',
            r'\{\{#each plans\.long_term_goals\}\}(.*?)\{\{/each\}\}'
        ]
        
        for pattern in plans_patterns:
            def replace_plans(match):
                loop_content = match.group(1)
                # æå–è®¡åˆ’ç±»å‹
                if "short_term_goals" in pattern:
                    plans = data.get("plans", {}).get("short_term_goals", [])
                elif "medium_term_goals" in pattern:
                    plans = data.get("plans", {}).get("medium_term_goals", [])
                elif "long_term_goals" in pattern:
                    plans = data.get("plans", {}).get("long_term_goals", [])
                else:
                    plans = []
                
                result = ""
                for i, plan in enumerate(plans):
                    plan_content = loop_content
                    # æ›¿æ¢è®¡åˆ’ç›¸å…³å˜é‡
                    for key, value in plan.items():
                        placeholder = f"{{{{{key}}}}}"
                        plan_content = plan_content.replace(placeholder, str(value))
                    
                    # æ›¿æ¢ç´¢å¼•
                    plan_content = plan_content.replace("{{index}}", str(i + 1))
                    result += plan_content
                
                return result
            
            content = re.sub(pattern, replace_plans, content, flags=re.DOTALL)
        
        return content
    
    def fill_missing_placeholders(self, content: str, data: Dict[str, Any]) -> str:
        """å¡«å……ç¼ºå¤±çš„å ä½ç¬¦ - ç§»é™¤'ï¼ˆç”±é¢è°ˆè¡¥å……ï¼‰'"""
        # æŸ¥æ‰¾æ‰€æœ‰æœªæ›¿æ¢çš„å ä½ç¬¦
        placeholder_pattern = r'\{\{([^}]+)\}\}'
        placeholders = re.findall(placeholder_pattern, content)
        
        for placeholder in placeholders:
            # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨æ•°æ®ä¸­ï¼ˆåŒ…æ‹¬åµŒå¥—ç»“æ„ï¼‰
            if not self.is_placeholder_in_data(placeholder, data):
                # è®¾ç½®é»˜è®¤å€¼
                default_value = self.get_default_value(placeholder)
                content = content.replace(f"{{{{{placeholder}}}}}", default_value)
        
        # æ¸…ç†æ‰€æœ‰æ®‹ç•™çš„"ï¼ˆç”±é¢è°ˆè¡¥å……ï¼‰"
        content = re.sub(r'ï¼ˆç”±é¢è°ˆè¡¥å……ï¼‰', 'ï¼ˆå¾…å®¶é•¿ç¡®è®¤ï¼‰', content)
        content = re.sub(r'ï¼ˆTBDï¼‰', 'ï¼ˆå¾…å®¶é•¿ç¡®è®¤ï¼‰', content)
        content = re.sub(r'ï¼ˆTODOï¼‰', 'ï¼ˆå¾…å®¶é•¿ç¡®è®¤ï¼‰', content)
        
        return content
    
    def is_placeholder_in_data(self, placeholder: str, data: Dict[str, Any]) -> bool:
        """æ£€æŸ¥å ä½ç¬¦æ˜¯å¦åœ¨æ•°æ®ä¸­å­˜åœ¨"""
        # å¤„ç†åµŒå¥—å­—æ®µï¼ˆå¦‚ family.education_valuesï¼‰
        if '.' in placeholder:
            keys = placeholder.split('.')
            current = data
            try:
                for key in keys:
                    current = current[key]
                return True
            except (KeyError, TypeError):
                return False
        else:
            return placeholder in data
    
    def get_default_value(self, placeholder: str) -> str:
        """è·å–å ä½ç¬¦çš„é»˜è®¤å€¼ - ç§»é™¤'ï¼ˆç”±é¢è°ˆè¡¥å……ï¼‰'"""
        defaults = {
            "student_name": "Alex Chen",
            "age": "14å²",
            "grade": "Grade 8",
            "gpa": "3.8/4.0",
            "academic_level": "ä¼˜ç§€å­¦æœ¯åŸºç¡€",
            "test_scores": "SSAT 85th percentile",
            "competition_achievements": "æœºå™¨äººç«èµ›çœçº§äºŒç­‰å¥–",
            "leadership_positions": "ç§‘æŠ€éƒ¨å‰¯éƒ¨é•¿",
            "project_experiences": "ç¯ä¿ä¹‰å–æ´»åŠ¨ç»„ç»‡",
            "teamwork_examples": "è·¨å¹´çº§åˆä½œé¡¹ç›®",
            "impact_metrics": "30+å­¦ç”Ÿå‚ä¸ï¼Œ800åŠ å…ƒç­¹æ¬¾",
            "innovation_examples": "ç§‘å­¦å®éªŒä¸­çš„ç‹¬ç‰¹è§è§£",
            "responsibility_examples": "ç¯ä¿æ´»åŠ¨çš„æŒç»­å‚ä¸",
            "learning_ability": "è‡ªä¸»å­¦ä¹ å’Œé—®é¢˜è§£å†³",
            "adaptability": "è·¨æ–‡åŒ–ç¯å¢ƒé€‚åº”",
            "target_schools": "Upper Canada College, Havergal College, St. Andrew's College"
        }
        
        # ä¸å†ä½¿ç”¨"ï¼ˆç”±é¢è°ˆè¡¥å……ï¼‰"ï¼Œæ”¹ä¸ºä¿å®ˆæ¨æ–­æˆ–"ï¼ˆå¾…å®¶é•¿ç¡®è®¤ï¼‰"
        return defaults.get(placeholder, "ï¼ˆå¾…å®¶é•¿ç¡®è®¤ï¼‰")
    
    def generate_metadata(self, content: str, student_data: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆæŠ¥å‘Šå…ƒæ•°æ®"""
        char_count = self.length_controller.count_chinese_chars(content)
        page_count = self.length_controller.estimate_page_count(content)
        
        return {
            "student_name": student_data.get("name", "Alex Chen"),
            "generation_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "word_count": char_count,
            "page_count": page_count,
            "template_version": "final_report_v2.0",
            "generator_version": "enhanced_v1.0"
        }
    
    def export_report(self, report_result: Dict[str, Any], 
                     output_format: str = "all") -> Dict[str, str]:
        """
        å¯¼å‡ºæŠ¥å‘Š
        
        Args:
            report_result: æŠ¥å‘Šç”Ÿæˆç»“æœ
            output_format: è¾“å‡ºæ ¼å¼ ("markdown", "docx", "pdf", "all")
            
        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        if self.use_llm_pipeline and hasattr(self, 'llm_generator'):
            # ä½¿ç”¨LLMç”Ÿæˆå™¨çš„å¯¼å‡ºæ–¹æ³•
            return self.llm_generator.export_report(report_result, output_format)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿå¯¼å‡ºæ–¹æ³•
            return self.export_traditional_report(report_result, output_format)
    
    def export_traditional_report(self, report_result: Dict[str, Any], 
                                 output_format: str = "all") -> Dict[str, str]:
        """
        ä¼ ç»Ÿå¯¼å‡ºæ–¹æ³•ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        
        Args:
            report_result: æŠ¥å‘Šç”Ÿæˆç»“æœ
            output_format: è¾“å‡ºæ ¼å¼ ("markdown", "docx", "pdf", "all")
            
        Returns:
            å¯¼å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        student_name = report_result["metadata"]["student_name"]
        
        # åˆ›å»ºæ—¶é—´æˆ³å­ç›®å½•
        output_subdir = self.output_dir / timestamp
        output_subdir.mkdir(exist_ok=True)
        
        exported_files = {}
        
        try:
            # å¯¼å‡ºMarkdown
            if output_format in ["markdown", "all"]:
                md_path = output_subdir / f"{student_name}_å­¦æ ¡ç”³è¯·æŠ¥å‘Š_{timestamp}.md"
                with open(md_path, 'w', encoding='utf-8') as f:
                    f.write(report_result["content"])
                exported_files["markdown"] = str(md_path)
                logger.info(f"MarkdownæŠ¥å‘Šå·²å¯¼å‡º: {md_path}")
            
            # å¯¼å‡ºDOCX
            if output_format in ["docx", "all"]:
                docx_path = output_subdir / f"{student_name}_å­¦æ ¡ç”³è¯·æŠ¥å‘Š_{timestamp}.docx"
                self.export_to_docx(report_result["content"], docx_path)
                exported_files["docx"] = str(docx_path)
                logger.info(f"DOCXæŠ¥å‘Šå·²å¯¼å‡º: {docx_path}")
            
            # å¯¼å‡ºPDF
            if output_format in ["pdf", "all"]:
                pdf_path = output_subdir / f"{student_name}_å­¦æ ¡ç”³è¯·æŠ¥å‘Š_{timestamp}.pdf"
                self.export_to_pdf(report_result["content"], pdf_path)
                exported_files["pdf"] = str(pdf_path)
                logger.info(f"PDFæŠ¥å‘Šå·²å¯¼å‡º: {pdf_path}")
            
            # å¯¼å‡ºå…ƒæ•°æ®
            metadata_path = output_subdir / f"{student_name}_æŠ¥å‘Šå…ƒæ•°æ®_{timestamp}.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(report_result["metadata"], f, ensure_ascii=False, indent=2)
            exported_files["metadata"] = str(metadata_path)
            
            # å¯¼å‡ºé•¿åº¦åˆ†æ
            if "length_analysis" in report_result:
                length_report = self.length_controller.generate_length_report(report_result["content"])
                length_path = output_subdir / f"{student_name}_é•¿åº¦åˆ†æ_{timestamp}.md"
                with open(length_path, 'w', encoding='utf-8') as f:
                    f.write(length_report)
                exported_files["length_analysis"] = str(length_path)
            
        except Exception as e:
            logger.error(f"å¯¼å‡ºå¤±è´¥: {e}")
            raise
        
        return exported_files
    
    def export_to_docx(self, content: str, output_path: Path):
        """ä½¿ç”¨Pandocå¯¼å‡ºä¸ºDOCXæ ¼å¼ï¼Œå¦‚æœPandocä¸å¯ç”¨åˆ™å›é€€åˆ°python-docx"""
        try:
            import subprocess
            import tempfile
            import os
            
            # é¦–å…ˆå°è¯•ä½¿ç”¨Pandoc
            try:
                # åˆ›å»ºä¸´æ—¶Markdownæ–‡ä»¶
                with tempfile.NamedTemporaryFile(mode='w', suffix='.md', delete=False, encoding='utf-8') as temp_md:
                    # æ¸…ç†Markdownå†…å®¹ï¼Œå»é™¤ç›®å½•å’Œé“¾æ¥
                    cleaned_content = self.clean_markdown_for_pandoc(content)
                    temp_md.write(cleaned_content)
                    temp_md_path = temp_md.name
                
                # æ£€æŸ¥å‚è€ƒæ¨¡æ¿æ˜¯å¦å­˜åœ¨
                reference_doc = Path("config/templates/reference.docx")
                if not reference_doc.exists():
                    logger.warning("å‚è€ƒæ¨¡æ¿ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨é»˜è®¤æ ·å¼")
                    reference_doc = None
                
                # æ„å»ºPandocå‘½ä»¤
                pandoc_cmd = [
                    'pandoc',
                    '--from', 'markdown',
                    '--to', 'docx',
                    '--output', str(output_path),
                    temp_md_path
                ]
                
                # æ·»åŠ å‚è€ƒæ¨¡æ¿
                if reference_doc:
                    pandoc_cmd.extend(['--reference-doc', str(reference_doc)])
                
                # æ‰§è¡ŒPandocè½¬æ¢
                result = subprocess.run(pandoc_cmd, capture_output=True, text=True)
                
                if result.returncode != 0:
                    raise Exception(f"Pandocè½¬æ¢å¤±è´¥: {result.stderr}")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(temp_md_path)
                
                # åå¤„ç†ï¼šæ·»åŠ å°é¢é¡µã€ç›®å½•å’Œé™„å½•
                self.post_process_docx(output_path, content)
                
                logger.info(f"DOCXæŠ¥å‘Šå·²å¯¼å‡º (Pandoc): {output_path}")
                return
                
            except FileNotFoundError:
                logger.warning("Pandocæœªå®‰è£…ï¼Œå›é€€åˆ°python-docxæ–¹æ³•")
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                if 'temp_md_path' in locals():
                    try:
                        os.unlink(temp_md_path)
                    except:
                        pass
            
            # å›é€€åˆ°python-docxæ–¹æ³•
            self.export_to_docx_fallback(content, output_path)
            
        except Exception as e:
            logger.error(f"DOCXå¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def export_to_docx_fallback(self, content: str, output_path: Path):
        """å›é€€çš„DOCXå¯¼å‡ºæ–¹æ³•ï¼Œä½¿ç”¨python-docx"""
        try:
            from docx import Document
            from docx.shared import Pt, Inches
            from docx.enum.text import WD_ALIGN_PARAGRAPH
            from docx.enum.style import WD_STYLE_TYPE
            
            # åˆ›å»ºæ–‡æ¡£
            doc = Document()
            
            # è®¾ç½®ä¸“ä¸šæ ·å¼
            self.setup_professional_styles_fallback(doc)
            
            # æ·»åŠ å°é¢é¡µ
            self.add_cover_page_fallback(doc, content)
            
            # æ·»åŠ ç›®å½•é¡µ
            self.add_toc_page_fallback(doc)
            
            # è§£ææ­£æ–‡å†…å®¹
            self.parse_content_fallback(content, doc)
            
            # æ·»åŠ é™„å½•
            self.add_appendix_fallback(doc, content)
            
            # è®¾ç½®é¡µçœ‰é¡µè„š
            self.setup_headers_footers_fallback(doc, content)
            
            # ä¿å­˜æ–‡æ¡£
            doc.save(str(output_path))
            logger.info(f"DOCXæŠ¥å‘Šå·²å¯¼å‡º (python-docx): {output_path}")
            
        except Exception as e:
            logger.error(f"å›é€€DOCXå¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def setup_docx_styles(self, doc):
        """è®¾ç½®DOCXæ ·å¼"""
        try:
            from docx.shared import Inches, Pt, RGBColor
            from docx.enum.text import WD_ALIGN_PARAGRAPH
            from docx.enum.style import WD_STYLE_TYPE
            # ä¸»æ ‡é¢˜æ ·å¼
            title_style = doc.styles.add_style('CustomTitle', WD_STYLE_TYPE.PARAGRAPH)
            title_style.font.name = 'Microsoft YaHei'
            title_style.font.size = Pt(24)
            title_style.font.bold = True
            title_style.font.color.rgb = RGBColor(0, 51, 102)
            title_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER
            title_style.paragraph_format.space_after = Pt(12)
            
            # ä¸€çº§æ ‡é¢˜æ ·å¼
            heading1_style = doc.styles.add_style('CustomHeading1', WD_STYLE_TYPE.PARAGRAPH)
            heading1_style.font.name = 'Microsoft YaHei'
            heading1_style.font.size = Pt(18)
            heading1_style.font.bold = True
            heading1_style.font.color.rgb = RGBColor(0, 102, 204)
            heading1_style.paragraph_format.space_before = Pt(12)
            heading1_style.paragraph_format.space_after = Pt(6)
            
            # äºŒçº§æ ‡é¢˜æ ·å¼
            heading2_style = doc.styles.add_style('CustomHeading2', WD_STYLE_TYPE.PARAGRAPH)
            heading2_style.font.name = 'Microsoft YaHei'
            heading2_style.font.size = Pt(14)
            heading2_style.font.bold = True
            heading2_style.font.color.rgb = RGBColor(51, 51, 51)
            heading2_style.paragraph_format.space_before = Pt(8)
            heading2_style.paragraph_format.space_after = Pt(4)
            
            # æ­£æ–‡æ ·å¼
            body_style = doc.styles.add_style('CustomBody', WD_STYLE_TYPE.PARAGRAPH)
            body_style.font.name = 'Microsoft YaHei'
            body_style.font.size = Pt(11)
            body_style.paragraph_format.space_after = Pt(6)
            body_style.paragraph_format.line_spacing = 1.25
            
        except Exception as e:
            logger.warning(f"æ ·å¼è®¾ç½®è­¦å‘Š: {e}")
    
    def parse_markdown_to_docx(self, content: str, doc):
        """è§£æMarkdownå†…å®¹å¹¶æ·»åŠ åˆ°DOCXæ–‡æ¡£"""
        from docx.shared import Pt, Inches, RGBColor
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from docx.enum.style import WD_STYLE_TYPE
        from docx.oxml.shared import OxmlElement, qn
        
        # è®¾ç½®æ–‡æ¡£æ ·å¼
        self.setup_professional_styles(doc)
        
        # æ·»åŠ å°é¢é¡µ
        self.add_cover_page(doc, content)
        
        # æ·»åŠ ç›®å½•é¡µ
        self.add_table_of_contents(doc)
        
        # è§£ææ­£æ–‡å†…å®¹
        lines = content.split('\n')
        current_table = None
        in_table = False
        
        for line in lines:
            line = line.strip()
            
            if not line:
                if current_table:
                    current_table = None
                    in_table = False
                continue
            
            # è·³è¿‡ç›®å½•å’Œå°é¢ç›¸å…³å†…å®¹
            if any(keyword in line for keyword in ['ğŸ“‹ ç›®å½•', '## ğŸ“‹ å­¦ç”Ÿæ¦‚å†µ', '---']):
                continue
            
            # å¤„ç†æ ‡é¢˜
            if line.startswith('# '):
                # ä¸€çº§æ ‡é¢˜
                title_text = line[2:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                p = doc.add_paragraph(title_text, style='Heading 1')
                p.alignment = WD_ALIGN_PARAGRAPH.CENTER
                p.paragraph_format.space_before = Pt(18)
                p.paragraph_format.space_after = Pt(12)
            elif line.startswith('## '):
                # äºŒçº§æ ‡é¢˜
                title_text = line[3:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                p = doc.add_paragraph(title_text, style='Heading 2')
                p.alignment = WD_ALIGN_PARAGRAPH.LEFT
                p.paragraph_format.space_before = Pt(12)
                p.paragraph_format.space_after = Pt(6)
            elif line.startswith('### '):
                # ä¸‰çº§æ ‡é¢˜
                title_text = line[4:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                p = doc.add_paragraph(title_text, style='Heading 3')
                p.alignment = WD_ALIGN_PARAGRAPH.LEFT
                p.paragraph_format.space_before = Pt(6)
                p.paragraph_format.space_after = Pt(6)
            elif line.startswith('- '):
                # åˆ—è¡¨é¡¹
                list_text = line[2:].replace('**', '').strip()
                p = doc.add_paragraph(list_text, style='List Bullet')
            elif line.startswith('|'):
                # è¡¨æ ¼è¡Œ
                cells = [cell.strip().replace('**', '').replace('---:', '').replace('---', '') for cell in line.split('|')[1:-1]]
                if not in_table:
                    current_table = doc.add_table(rows=1, cols=len(cells))
                    current_table.style = 'Table Grid'
                    current_table.autofit = True
                    in_table = True
                    
                    # è¡¨å¤´
                    hdr_cells = current_table.rows[0].cells
                    for i, cell in enumerate(cells):
                        hdr_cells[i].text = cell
                        hdr_cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER
                        for run in hdr_cells[i].paragraphs[0].runs:
                            run.font.bold = True
                else:
                    row_cells = current_table.add_row().cells
                    for i, cell in enumerate(cells):
                        row_cells[i].text = cell
                        row_cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.LEFT
            else:
                # æ™®é€šæ®µè½
                if current_table:
                    current_table = None
                    in_table = False
                
                # å¤„ç†ç²—ä½“æ–‡æœ¬
                paragraph_text = line.replace('**', '').replace('*', '').strip()
                if paragraph_text:
                    p = doc.add_paragraph(paragraph_text, style='Normal')
                    p.paragraph_format.first_line_indent = Inches(0.5)  # é¦–è¡Œç¼©è¿›2å­—ç¬¦
                    
                    # å¤„ç†å…³é”®å­—åŠ ç²—ï¼ˆå¦‚"å®¶åº­ä»·å€¼è§‚"ã€"GPA"ç­‰ï¼‰
                    self.format_keywords(p)
        
        # æ·»åŠ æŠ¥å‘Šä¿¡æ¯é¡µ
        self.add_report_info_page(doc)
        
        # è®¾ç½®é¡µçœ‰é¡µè„š
        self.setup_headers_footers(doc, content)
    
    def export_to_pdf(self, content: str, output_path: Path):
        """å¯¼å‡ºä¸ºPDFæ ¼å¼"""
        try:
            import markdown
            from weasyprint import HTML, CSS
            
            # å°†Markdownè½¬æ¢ä¸ºHTML
            html_content = markdown.markdown(content, extensions=['tables', 'fenced_code'])
            
            # æ·»åŠ CSSæ ·å¼
            css_content = """
            body {
                font-family: 'Microsoft YaHei', Arial, sans-serif;
                font-size: 11pt;
                line-height: 1.25;
                margin: 2cm;
            }
            h1 {
                color: #003366;
                font-size: 24pt;
                text-align: center;
                margin-bottom: 12pt;
            }
            h2 {
                color: #0066cc;
                font-size: 18pt;
                margin-top: 12pt;
                margin-bottom: 6pt;
            }
            h3 {
                color: #333333;
                font-size: 14pt;
                margin-top: 8pt;
                margin-bottom: 4pt;
            }
            table {
                width: 100%;
                border-collapse: collapse;
                margin: 10pt 0;
            }
            th, td {
                border: 1px solid #ddd;
                padding: 8pt;
                text-align: left;
            }
            th {
                background-color: #f5f5f5;
                font-weight: bold;
            }
            """
            
            # ç”Ÿæˆå®Œæ•´HTML
            full_html = f"""
            <!DOCTYPE html>
            <html>
            <head>
                <meta charset="utf-8">
                <style>{css_content}</style>
            </head>
            <body>
                {html_content}
            </body>
            </html>
            """
            
            # è½¬æ¢ä¸ºPDF
            HTML(string=full_html).write_pdf(str(output_path))
            
        except ImportError:
            logger.error("éœ€è¦å®‰è£…ç›¸å…³åº“: pip install markdown weasyprint")
            raise
        except Exception as e:
            logger.error(f"PDFå¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    def setup_professional_styles(self, doc):
        """è®¾ç½®ä¸“ä¸šWordæ ·å¼"""
        from docx.shared import Pt
        from docx.enum.style import WD_STYLE_TYPE
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # ä¸€çº§æ ‡é¢˜æ ·å¼
        try:
            heading1_style = doc.styles.add_style('Heading 1', WD_STYLE_TYPE.PARAGRAPH)
            heading1_style.font.name = 'é»‘ä½“'
            heading1_style.font.size = Pt(16)
            heading1_style.font.bold = True
            heading1_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER
            heading1_style.paragraph_format.space_before = Pt(18)
            heading1_style.paragraph_format.space_after = Pt(12)
        except:
            pass  # æ ·å¼å¯èƒ½å·²å­˜åœ¨
        
        # äºŒçº§æ ‡é¢˜æ ·å¼
        try:
            heading2_style = doc.styles.add_style('Heading 2', WD_STYLE_TYPE.PARAGRAPH)
            heading2_style.font.name = 'é»‘ä½“'
            heading2_style.font.size = Pt(14)
            heading2_style.font.bold = True
            heading2_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.LEFT
            heading2_style.paragraph_format.space_before = Pt(12)
            heading2_style.paragraph_format.space_after = Pt(6)
        except:
            pass
        
        # ä¸‰çº§æ ‡é¢˜æ ·å¼
        try:
            heading3_style = doc.styles.add_style('Heading 3', WD_STYLE_TYPE.PARAGRAPH)
            heading3_style.font.name = 'é»‘ä½“'
            heading3_style.font.size = Pt(12)
            heading3_style.font.bold = True
            heading3_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.LEFT
            heading3_style.paragraph_format.space_before = Pt(6)
            heading3_style.paragraph_format.space_after = Pt(6)
        except:
            pass
        
        # æ­£æ–‡æ ·å¼
        try:
            normal_style = doc.styles.add_style('Normal', WD_STYLE_TYPE.PARAGRAPH)
            normal_style.font.name = 'ä»¿å®‹'
            normal_style.font.size = Pt(12)
            normal_style.paragraph_format.line_spacing = 1.5
            normal_style.paragraph_format.first_line_indent = Pt(24)  # é¦–è¡Œç¼©è¿›2å­—ç¬¦
        except:
            pass
    
    def add_cover_page(self, doc, content):
        """æ·»åŠ å°é¢é¡µ"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # æå–å­¦ç”Ÿå§“å
        student_name = "Alex Chen"  # é»˜è®¤å€¼
        for line in content.split('\n'):
            if 'å§“å' in line and 'Alex Chen' in line:
                student_name = "Alex Chen"
                break
        
        # æ·»åŠ å°é¢æ ‡é¢˜
        title = doc.add_paragraph(f"{student_name} å­¦æ ¡ç”³è¯·æŠ¥å‘Š")
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        title.runs[0].font.name = 'é»‘ä½“'
        title.runs[0].font.size = Pt(24)
        title.runs[0].font.bold = True
        title.paragraph_format.space_after = Pt(24)
        
        # æ·»åŠ å‰¯æ ‡é¢˜
        subtitle = doc.add_paragraph("ç§ç«‹å­¦æ ¡ç”³è¯·å’¨è¯¢æŠ¥å‘Š")
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.runs[0].font.name = 'ä»¿å®‹'
        subtitle.runs[0].font.size = Pt(16)
        subtitle.paragraph_format.space_after = Pt(36)
        
        # æ·»åŠ æ—¥æœŸå’Œé¡¾é—®ä¿¡æ¯
        from datetime import datetime
        date_info = doc.add_paragraph(f"æŠ¥å‘Šæ—¥æœŸï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
        date_info.alignment = WD_ALIGN_PARAGRAPH.CENTER
        date_info.runs[0].font.name = 'ä»¿å®‹'
        date_info.runs[0].font.size = Pt(12)
        
        consultant_info = doc.add_paragraph("ä¸“ä¸šé¡¾é—®ï¼šç§æ ¡ç”³è¯·ä¸“å®¶å›¢é˜Ÿ")
        consultant_info.alignment = WD_ALIGN_PARAGRAPH.CENTER
        consultant_info.runs[0].font.name = 'ä»¿å®‹'
        consultant_info.runs[0].font.size = Pt(12)
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
    
    def add_table_of_contents(self, doc):
        """æ·»åŠ ç›®å½•é¡µ"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # ç›®å½•æ ‡é¢˜
        toc_title = doc.add_paragraph("ç›®å½•")
        toc_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        toc_title.runs[0].font.name = 'é»‘ä½“'
        toc_title.runs[0].font.size = Pt(16)
        toc_title.runs[0].font.bold = True
        toc_title.paragraph_format.space_after = Pt(18)
        
        # ç›®å½•é¡¹
        toc_items = [
            "1. å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯",
            "2. å­¦æ ¡ç”³è¯·å®šä½", 
            "3. å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦",
            "4. å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡",
            "5. ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥",
            "6. å½•å–åå»¶ä¼¸å»ºè®®"
        ]
        
        for item in toc_items:
            toc_item = doc.add_paragraph(item)
            toc_item.runs[0].font.name = 'ä»¿å®‹'
            toc_item.runs[0].font.size = Pt(12)
            toc_item.paragraph_format.space_after = Pt(6)
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
    
    def format_keywords(self, paragraph):
        """æ ¼å¼åŒ–å…³é”®å­—åŠ ç²—"""
        keywords = ['å®¶åº­ä»·å€¼è§‚', 'æ•™è‚²ç›®æ ‡', 'æ–‡åŒ–èƒŒæ™¯', 'æ”¯æŒç¨‹åº¦', 'æœŸæœ›è®¾å®š', 'èµ„æºæŠ•å…¥',
                   'GPA', 'å¼ºé¡¹é¢†åŸŸ', 'æ ‡å‡†åŒ–è€ƒè¯•', 'å­¦æœ¯ç«èµ›', 'å­¦ä¹ èƒ½åŠ›', 'é€‚åº”èƒ½åŠ›',
                   'åˆ›æ–°æ€ç»´', 'è´£ä»»æ„Ÿ', 'å›¢é˜Ÿåä½œ', 'å½±å“åŠ›', 'é¡¹ç›®ç»éªŒ', 'ä¸“ä¸šç‰¹é•¿',
                   'å…¨çƒè§†é‡', 'åˆ›æ–°ç²¾ç¥', 'å…¨äººå‘å±•', 'å­¦æœ¯', 'æ´»åŠ¨èµ„æº', 'ä»·å€¼è§‚', 'æ–‡åŒ–',
                   'æ€§æ ¼', 'æ°›å›´', 'ä¼˜åŠ¿åŒ¹é…', 'ç”³è¯·å»ºè®®', 'æ¨èç†ç”±']
        
        text = paragraph.text
        for keyword in keywords:
            if keyword in text:
                # ç®€å•çš„å…³é”®å­—åŠ ç²—å¤„ç†
                runs = paragraph.runs
                for run in runs:
                    if keyword in run.text:
                        run.font.bold = True
                        break
    
    def add_report_info_page(self, doc):
        """æ·»åŠ æŠ¥å‘Šä¿¡æ¯é¡µ"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from datetime import datetime
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
        
        # æŠ¥å‘Šä¿¡æ¯æ ‡é¢˜
        info_title = doc.add_paragraph("æŠ¥å‘Šä¿¡æ¯")
        info_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        info_title.runs[0].font.name = 'é»‘ä½“'
        info_title.runs[0].font.size = Pt(16)
        info_title.runs[0].font.bold = True
        info_title.paragraph_format.space_after = Pt(18)
        
        # æŠ¥å‘Šä¿¡æ¯å†…å®¹
        info_items = [
            f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}",
            "ä¸“ä¸šé¡¾é—®ï¼šç§æ ¡ç”³è¯·ä¸“å®¶å›¢é˜Ÿ",
            "ç‰ˆæœ¬ï¼šv2.0",
            "é¡µæ•°ç»Ÿè®¡ï¼š{PAGE} é¡µ",
            "å­—æ•°ç»Ÿè®¡ï¼š{NUMCHARS} å­—"
        ]
        
        for item in info_items:
            info_item = doc.add_paragraph(item)
            info_item.runs[0].font.name = 'ä»¿å®‹'
            info_item.runs[0].font.size = Pt(12)
            info_item.paragraph_format.space_after = Pt(6)
    
    def setup_headers_footers(self, doc, content):
        """è®¾ç½®é¡µçœ‰é¡µè„š"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # æå–å­¦ç”Ÿå§“å
        student_name = "Alex Chen"
        for line in content.split('\n'):
            if 'å§“å' in line and 'Alex Chen' in line:
                student_name = "Alex Chen"
                break
        
        # è®¾ç½®é¡µçœ‰
        for section in doc.sections:
            header = section.header
            header_para = header.paragraphs[0]
            header_para.text = f"{student_name} - å­¦æ ¡ç”³è¯·æŠ¥å‘Š"
            header_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            header_para.runs[0].font.name = 'ä»¿å®‹'
            header_para.runs[0].font.size = Pt(10.5)
            
            # è®¾ç½®é¡µè„š
            footer = section.footer
            footer_para = footer.paragraphs[0]
            footer_para.text = "ç¬¬ {PAGE} é¡µ / å…± {NUMPAGES} é¡µ"
            footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            footer_para.runs[0].font.name = 'ä»¿å®‹'
            footer_para.runs[0].font.size = Pt(10.5)
    
    def clean_markdown_for_pandoc(self, content: str) -> str:
        """æ¸…ç†Markdownå†…å®¹ï¼Œå»é™¤ç›®å½•å’Œé“¾æ¥ï¼Œå‡†å¤‡Pandocè½¬æ¢"""
        lines = content.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # è·³è¿‡ç›®å½•éƒ¨åˆ†
            if any(keyword in line for keyword in ['ğŸ“‹ ç›®å½•', '## ğŸ“‹ å­¦ç”Ÿæ¦‚å†µ', '---']):
                continue
            
            # è·³è¿‡Markdowné“¾æ¥æ ¼å¼çš„ç›®å½•é¡¹
            if line.startswith(('1. [', '2. [', '3. [', '4. [', '5. [', '6. [')):
                continue
            
            # æ¸…ç†æ ‡é¢˜ä¸­çš„emojiå’Œç‰¹æ®Šå­—ç¬¦
            if line.startswith('# '):
                cleaned_line = line[2:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                cleaned_lines.append(f"# {cleaned_line}")
            elif line.startswith('## '):
                cleaned_line = line[3:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                cleaned_lines.append(f"## {cleaned_line}")
            elif line.startswith('### '):
                cleaned_line = line[4:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                cleaned_lines.append(f"### {cleaned_line}")
            elif line:
                # ä¿ç•™å…¶ä»–å†…å®¹
                cleaned_lines.append(line)
        
        return '\n'.join(cleaned_lines)
    
    def post_process_docx(self, docx_path: Path, original_content: str):
        """åå¤„ç†DOCXæ–‡ä»¶ï¼šæ·»åŠ å°é¢é¡µã€ç›®å½•å’Œé™„å½•"""
        try:
            from docx import Document
            from docx.shared import Pt
            from docx.enum.text import WD_ALIGN_PARAGRAPH
            from docx.enum.section import WD_SECTION
            from datetime import datetime
            
            # æ‰“å¼€è½¬æ¢åçš„æ–‡æ¡£
            doc = Document(str(docx_path))
            
            # æå–å­¦ç”Ÿå§“å
            student_name = "Alex Chen"
            for line in original_content.split('\n'):
                if 'å§“å' in line and 'Alex Chen' in line:
                    student_name = "Alex Chen"
                    break
            
            # åœ¨æ–‡æ¡£å¼€å¤´æ·»åŠ å°é¢é¡µ
            self.add_cover_page_to_doc(doc, student_name)
            
            # æ·»åŠ ç›®å½•é¡µ
            self.add_toc_page_to_doc(doc)
            
            # åœ¨æ–‡æ¡£æœ«å°¾æ·»åŠ é™„å½•
            self.add_appendix_to_doc(doc, original_content)
            
            # è®¾ç½®é¡µçœ‰é¡µè„š
            self.setup_headers_footers_for_doc(doc, student_name)
            
            # ä¿å­˜ä¿®æ”¹åçš„æ–‡æ¡£
            doc.save(str(docx_path))
            
        except Exception as e:
            logger.warning(f"DOCXåå¤„ç†å¤±è´¥: {e}")
    
    def add_cover_page_to_doc(self, doc, student_name: str):
        """åœ¨æ–‡æ¡£å¼€å¤´æ·»åŠ å°é¢é¡µ"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from datetime import datetime
        
        # åœ¨æ–‡æ¡£å¼€å¤´æ’å…¥å°é¢å†…å®¹
        cover_paragraphs = []
        
        # ä¸»æ ‡é¢˜
        title = doc.paragraphs[0]._element
        title.text = f"{student_name} å­¦æ ¡ç”³è¯·æŠ¥å‘Š"
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        title.runs[0].font.name = 'é»‘ä½“'
        title.runs[0].font.size = Pt(24)
        title.runs[0].font.bold = True
        
        # å‰¯æ ‡é¢˜
        subtitle = doc.add_paragraph("ç§ç«‹å­¦æ ¡ç”³è¯·å’¨è¯¢æŠ¥å‘Š")
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.runs[0].font.name = 'ä»¿å®‹'
        subtitle.runs[0].font.size = Pt(16)
        
        # æ—¥æœŸå’Œé¡¾é—®ä¿¡æ¯
        date_info = doc.add_paragraph(f"æŠ¥å‘Šæ—¥æœŸï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
        date_info.alignment = WD_ALIGN_PARAGRAPH.CENTER
        date_info.runs[0].font.name = 'ä»¿å®‹'
        date_info.runs[0].font.size = Pt(12)
        
        consultant_info = doc.add_paragraph("ä¸“ä¸šé¡¾é—®ï¼šç§æ ¡ç”³è¯·ä¸“å®¶å›¢é˜Ÿ")
        consultant_info.alignment = WD_ALIGN_PARAGRAPH.CENTER
        consultant_info.runs[0].font.name = 'ä»¿å®‹'
        consultant_info.runs[0].font.size = Pt(12)
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
    
    def add_toc_page_to_doc(self, doc):
        """æ·»åŠ ç›®å½•é¡µ"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # ç›®å½•æ ‡é¢˜
        toc_title = doc.add_paragraph("ç›®å½•")
        toc_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        toc_title.runs[0].font.name = 'é»‘ä½“'
        toc_title.runs[0].font.size = Pt(16)
        toc_title.runs[0].font.bold = True
        
        # ç›®å½•è¯´æ˜
        toc_note = doc.add_paragraph("è¯·ä½¿ç”¨Wordçš„\"å¼•ç”¨\"â†’\"ç›®å½•\"â†’\"è‡ªåŠ¨ç›®å½•\"åŠŸèƒ½ç”Ÿæˆç›®å½•")
        toc_note.runs[0].font.name = 'ä»¿å®‹'
        toc_note.runs[0].font.size = Pt(12)
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
    
    def add_appendix_to_doc(self, doc, original_content: str):
        """åœ¨æ–‡æ¡£æœ«å°¾æ·»åŠ é™„å½•"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from datetime import datetime
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
        
        # é™„å½•æ ‡é¢˜
        appendix_title = doc.add_paragraph("é™„å½•")
        appendix_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        appendix_title.runs[0].font.name = 'é»‘ä½“'
        appendix_title.runs[0].font.size = Pt(16)
        appendix_title.runs[0].font.bold = True
        
        # æŠ¥å‘Šä¿¡æ¯
        info_items = [
            f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}",
            "ä¸“ä¸šé¡¾é—®ï¼šç§æ ¡ç”³è¯·ä¸“å®¶å›¢é˜Ÿ",
            "ç‰ˆæœ¬ï¼šv2.0",
            "é¡µæ•°ç»Ÿè®¡ï¼š{PAGE} é¡µ",
            "å­—æ•°ç»Ÿè®¡ï¼š{NUMCHARS} å­—"
        ]
        
        for item in info_items:
            info_item = doc.add_paragraph(item)
            info_item.runs[0].font.name = 'ä»¿å®‹'
            info_item.runs[0].font.size = Pt(12)
    
    def setup_headers_footers_for_doc(self, doc, student_name: str):
        """ä¸ºæ–‡æ¡£è®¾ç½®é¡µçœ‰é¡µè„š"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # è®¾ç½®é¡µçœ‰é¡µè„š
        for section in doc.sections:
            # é¡µçœ‰
            header = section.header
            header_para = header.paragraphs[0]
            header_para.text = f"{student_name} - å­¦æ ¡ç”³è¯·æŠ¥å‘Š"
            header_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            header_para.runs[0].font.name = 'ä»¿å®‹'
            header_para.runs[0].font.size = Pt(10.5)
            
            # é¡µè„š
            footer = section.footer
            footer_para = footer.paragraphs[0]
            footer_para.text = "ç¬¬ {PAGE} é¡µ / å…± {NUMPAGES} é¡µ"
            footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            footer_para.runs[0].font.name = 'ä»¿å®‹'
            footer_para.runs[0].font.size = Pt(10.5)
    
    def setup_professional_styles_fallback(self, doc):
        """è®¾ç½®ä¸“ä¸šæ ·å¼ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        from docx.shared import Pt
        from docx.enum.style import WD_STYLE_TYPE
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # å®šä¹‰Heading 1æ ·å¼
        try:
            heading1_style = doc.styles.add_style('Heading 1', WD_STYLE_TYPE.PARAGRAPH)
            heading1_style.font.name = 'é»‘ä½“'
            heading1_style.font.size = Pt(16)
            heading1_style.font.bold = True
            heading1_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.CENTER
            heading1_style.paragraph_format.space_before = Pt(18)
            heading1_style.paragraph_format.space_after = Pt(12)
        except:
            pass
        
        # å®šä¹‰Heading 2æ ·å¼
        try:
            heading2_style = doc.styles.add_style('Heading 2', WD_STYLE_TYPE.PARAGRAPH)
            heading2_style.font.name = 'é»‘ä½“'
            heading2_style.font.size = Pt(14)
            heading2_style.font.bold = True
            heading2_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.LEFT
            heading2_style.paragraph_format.space_before = Pt(12)
            heading2_style.paragraph_format.space_after = Pt(6)
        except:
            pass
        
        # å®šä¹‰Heading 3æ ·å¼
        try:
            heading3_style = doc.styles.add_style('Heading 3', WD_STYLE_TYPE.PARAGRAPH)
            heading3_style.font.name = 'é»‘ä½“'
            heading3_style.font.size = Pt(12)
            heading3_style.font.bold = True
            heading3_style.paragraph_format.alignment = WD_ALIGN_PARAGRAPH.LEFT
            heading3_style.paragraph_format.space_before = Pt(6)
            heading3_style.paragraph_format.space_after = Pt(6)
        except:
            pass
        
        # å®šä¹‰æ­£æ–‡æ ·å¼
        try:
            normal_style = doc.styles.add_style('Normal', WD_STYLE_TYPE.PARAGRAPH)
            normal_style.font.name = 'ä»¿å®‹'
            normal_style.font.size = Pt(12)
            normal_style.paragraph_format.line_spacing = 1.5
            normal_style.paragraph_format.first_line_indent = Pt(24)  # é¦–è¡Œç¼©è¿›2å­—ç¬¦
        except:
            pass
    
    def add_cover_page_fallback(self, doc, content: str):
        """æ·»åŠ å°é¢é¡µï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from datetime import datetime
        
        # æå–å­¦ç”Ÿå§“å
        student_name = "Alex Chen"
        for line in content.split('\n'):
            if 'å§“å' in line and 'Alex Chen' in line:
                student_name = "Alex Chen"
                break
        
        # æ·»åŠ å°é¢æ ‡é¢˜
        title = doc.add_paragraph(f"{student_name} å­¦æ ¡ç”³è¯·æŠ¥å‘Š")
        title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        title.runs[0].font.name = 'é»‘ä½“'
        title.runs[0].font.size = Pt(24)
        title.runs[0].font.bold = True
        title.paragraph_format.space_after = Pt(24)
        
        # æ·»åŠ å‰¯æ ‡é¢˜
        subtitle = doc.add_paragraph("ç§ç«‹å­¦æ ¡ç”³è¯·å’¨è¯¢æŠ¥å‘Š")
        subtitle.alignment = WD_ALIGN_PARAGRAPH.CENTER
        subtitle.runs[0].font.name = 'ä»¿å®‹'
        subtitle.runs[0].font.size = Pt(16)
        subtitle.paragraph_format.space_after = Pt(36)
        
        # æ·»åŠ æ—¥æœŸå’Œé¡¾é—®ä¿¡æ¯
        date_info = doc.add_paragraph(f"æŠ¥å‘Šæ—¥æœŸï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥')}")
        date_info.alignment = WD_ALIGN_PARAGRAPH.CENTER
        date_info.runs[0].font.name = 'ä»¿å®‹'
        date_info.runs[0].font.size = Pt(12)
        
        consultant_info = doc.add_paragraph("ä¸“ä¸šé¡¾é—®ï¼šç§æ ¡ç”³è¯·ä¸“å®¶å›¢é˜Ÿ")
        consultant_info.alignment = WD_ALIGN_PARAGRAPH.CENTER
        consultant_info.runs[0].font.name = 'ä»¿å®‹'
        consultant_info.runs[0].font.size = Pt(12)
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
    
    def add_toc_page_fallback(self, doc):
        """æ·»åŠ ç›®å½•é¡µï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # ç›®å½•æ ‡é¢˜
        toc_title = doc.add_paragraph("ç›®å½•")
        toc_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        toc_title.runs[0].font.name = 'é»‘ä½“'
        toc_title.runs[0].font.size = Pt(16)
        toc_title.runs[0].font.bold = True
        toc_title.paragraph_format.space_after = Pt(18)
        
        # ç›®å½•è¯´æ˜
        toc_note = doc.add_paragraph("è¯·ä½¿ç”¨Wordçš„\"å¼•ç”¨\"â†’\"ç›®å½•\"â†’\"è‡ªåŠ¨ç›®å½•\"åŠŸèƒ½ç”Ÿæˆç›®å½•")
        toc_note.runs[0].font.name = 'ä»¿å®‹'
        toc_note.runs[0].font.size = Pt(12)
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
    
    def parse_content_fallback(self, content: str, doc):
        """è§£æå†…å®¹ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        lines = content.split('\n')
        current_table = None
        in_table = False
        
        for line in lines:
            line = line.strip()
            
            if not line:
                if current_table:
                    current_table = None
                    in_table = False
                continue
            
            # è·³è¿‡ç›®å½•éƒ¨åˆ†
            if any(keyword in line for keyword in ['ğŸ“‹ ç›®å½•', '## ğŸ“‹ å­¦ç”Ÿæ¦‚å†µ', '---']):
                continue
            
            # è·³è¿‡Markdowné“¾æ¥æ ¼å¼çš„ç›®å½•é¡¹
            if line.startswith(('1. [', '2. [', '3. [', '4. [', '5. [', '6. [')):
                continue
            
            # å¤„ç†æ ‡é¢˜
            if line.startswith('# '):
                # ä¸€çº§æ ‡é¢˜
                title_text = line[2:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                p = doc.add_paragraph(title_text, style='Heading 1')
                p.alignment = WD_ALIGN_PARAGRAPH.CENTER
            elif line.startswith('## '):
                # äºŒçº§æ ‡é¢˜
                title_text = line[3:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                p = doc.add_paragraph(title_text, style='Heading 2')
                p.alignment = WD_ALIGN_PARAGRAPH.LEFT
            elif line.startswith('### '):
                # ä¸‰çº§æ ‡é¢˜
                title_text = line[4:].replace('ğŸ¯', '').replace('ğŸ“‹', '').replace('ğŸ‘¨â€ğŸ‘©â€ğŸ‘§â€ğŸ‘¦', '').replace('ğŸ«', '').replace('ğŸ“š', '').replace('ğŸ“…', '').replace('ğŸ“', '').strip()
                p = doc.add_paragraph(title_text, style='Heading 3')
                p.alignment = WD_ALIGN_PARAGRAPH.LEFT
            elif line.startswith('- '):
                # åˆ—è¡¨é¡¹
                list_text = line[2:].replace('**', '').strip()
                p = doc.add_paragraph(list_text, style='List Bullet')
            elif line.startswith('|'):
                # è¡¨æ ¼è¡Œ
                cells = [cell.strip().replace('**', '').replace('---:', '').replace('---', '') for cell in line.split('|')[1:-1]]
                if not in_table:
                    current_table = doc.add_table(rows=1, cols=len(cells))
                    current_table.style = 'Table Grid'
                    current_table.autofit = True
                    in_table = True
                    
                    # è¡¨å¤´
                    hdr_cells = current_table.rows[0].cells
                    for i, cell in enumerate(cells):
                        hdr_cells[i].text = cell
                        hdr_cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.CENTER
                        for run in hdr_cells[i].paragraphs[0].runs:
                            run.font.bold = True
                else:
                    row_cells = current_table.add_row().cells
                    for i, cell in enumerate(cells):
                        row_cells[i].text = cell
                        row_cells[i].paragraphs[0].alignment = WD_ALIGN_PARAGRAPH.LEFT
            else:
                # æ™®é€šæ®µè½
                if current_table:
                    current_table = None
                    in_table = False
                
                # å¤„ç†ç²—ä½“æ–‡æœ¬
                paragraph_text = line.replace('**', '').replace('*', '').strip()
                if paragraph_text:
                    p = doc.add_paragraph(paragraph_text, style='Normal')
                    p.paragraph_format.first_line_indent = Pt(24)  # é¦–è¡Œç¼©è¿›2å­—ç¬¦
                    
                    # å¤„ç†å…³é”®å­—åŠ ç²—
                    self.format_keywords_fallback(p)
    
    def add_appendix_fallback(self, doc, content: str):
        """æ·»åŠ é™„å½•ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        from datetime import datetime
        
        # æ·»åŠ åˆ†é¡µç¬¦
        doc.add_page_break()
        
        # é™„å½•æ ‡é¢˜
        appendix_title = doc.add_paragraph("é™„å½•")
        appendix_title.alignment = WD_ALIGN_PARAGRAPH.CENTER
        appendix_title.runs[0].font.name = 'é»‘ä½“'
        appendix_title.runs[0].font.size = Pt(16)
        appendix_title.runs[0].font.bold = True
        appendix_title.paragraph_format.space_after = Pt(18)
        
        # æŠ¥å‘Šä¿¡æ¯
        info_items = [
            f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´ï¼š{datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M')}",
            "ä¸“ä¸šé¡¾é—®ï¼šç§æ ¡ç”³è¯·ä¸“å®¶å›¢é˜Ÿ",
            "ç‰ˆæœ¬ï¼šv2.0",
            "é¡µæ•°ç»Ÿè®¡ï¼š{PAGE} é¡µ",
            "å­—æ•°ç»Ÿè®¡ï¼š{NUMCHARS} å­—"
        ]
        
        for item in info_items:
            info_item = doc.add_paragraph(item)
            info_item.runs[0].font.name = 'ä»¿å®‹'
            info_item.runs[0].font.size = Pt(12)
            info_item.paragraph_format.space_after = Pt(6)
    
    def setup_headers_footers_fallback(self, doc, content: str):
        """è®¾ç½®é¡µçœ‰é¡µè„šï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        from docx.shared import Pt
        from docx.enum.text import WD_ALIGN_PARAGRAPH
        
        # æå–å­¦ç”Ÿå§“å
        student_name = "Alex Chen"
        for line in content.split('\n'):
            if 'å§“å' in line and 'Alex Chen' in line:
                student_name = "Alex Chen"
                break
        
        # è®¾ç½®é¡µçœ‰é¡µè„š
        for section in doc.sections:
            # é¡µçœ‰
            header = section.header
            header_para = header.paragraphs[0]
            header_para.text = f"{student_name} - å­¦æ ¡ç”³è¯·æŠ¥å‘Š"
            header_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            header_para.runs[0].font.name = 'ä»¿å®‹'
            header_para.runs[0].font.size = Pt(10.5)
            
            # é¡µè„š
            footer = section.footer
            footer_para = footer.paragraphs[0]
            footer_para.text = "ç¬¬ {PAGE} é¡µ / å…± {NUMPAGES} é¡µ"
            footer_para.alignment = WD_ALIGN_PARAGRAPH.CENTER
            footer_para.runs[0].font.name = 'ä»¿å®‹'
            footer_para.runs[0].font.size = Pt(10.5)
    
    def format_keywords_fallback(self, paragraph):
        """æ ¼å¼åŒ–å…³é”®å­—åŠ ç²—ï¼ˆå›é€€æ–¹æ³•ï¼‰"""
        keywords = ['å®¶åº­ä»·å€¼è§‚', 'æ•™è‚²ç›®æ ‡', 'æ–‡åŒ–èƒŒæ™¯', 'æ”¯æŒç¨‹åº¦', 'æœŸæœ›è®¾å®š', 'èµ„æºæŠ•å…¥',
                   'GPA', 'å¼ºé¡¹é¢†åŸŸ', 'æ ‡å‡†åŒ–è€ƒè¯•', 'å­¦æœ¯ç«èµ›', 'å­¦ä¹ èƒ½åŠ›', 'é€‚åº”èƒ½åŠ›',
                   'åˆ›æ–°æ€ç»´', 'è´£ä»»æ„Ÿ', 'å›¢é˜Ÿåä½œ', 'å½±å“åŠ›', 'é¡¹ç›®ç»éªŒ', 'ä¸“ä¸šç‰¹é•¿',
                   'å…¨çƒè§†é‡', 'åˆ›æ–°ç²¾ç¥', 'å…¨äººå‘å±•', 'å­¦æœ¯', 'æ´»åŠ¨èµ„æº', 'ä»·å€¼è§‚', 'æ–‡åŒ–',
                   'æ€§æ ¼', 'æ°›å›´', 'ä¼˜åŠ¿åŒ¹é…', 'ç”³è¯·å»ºè®®', 'æ¨èç†ç”±']
        
        text = paragraph.text
        for keyword in keywords:
            if keyword in text:
                # ç®€å•çš„å…³é”®å­—åŠ ç²—å¤„ç†
                runs = paragraph.runs
                for run in runs:
                    if keyword in run.text:
                        run.font.bold = True
                        break

def main():
    """æµ‹è¯•å¢å¼ºç‰ˆæŠ¥å‘Šç”Ÿæˆå™¨"""
    # æµ‹è¯•LLM pipeline
    print("=== æµ‹è¯•LLMé©±åŠ¨pipeline ===")
    generator_llm = EnhancedReportGenerator(use_llm_pipeline=True)
    
    # æµ‹è¯•æ•°æ®
    student_data = {
        "name": "Alex Chen",
        "age": "14å²",
        "grade": "Grade 8",
        "gpa": "3.8/4.0",
        "academic_strengths": "æ•°å­¦ã€ç‰©ç†ã€è®¡ç®—æœºç§‘å­¦",
        "competition_achievements": "æœºå™¨äººç«èµ›çœçº§äºŒç­‰å¥–",
        "leadership_positions": "ç§‘æŠ€éƒ¨å‰¯éƒ¨é•¿",
        "project_experiences": "ç¯ä¿ä¹‰å–æ´»åŠ¨ç»„ç»‡",
        "learning_ability": "è‡ªä¸»å­¦ä¹ å’Œé—®é¢˜è§£å†³",
        "adaptability": "è·¨æ–‡åŒ–ç¯å¢ƒé€‚åº”",
        "target_schools": [
            {
                "name": "Upper Canada College",
                "scores": {"academic": 4, "activities": 4, "culture": 5, "personality": 4},
                "weights": {"academic": 0.35, "activities": 0.25, "culture": 0.2, "personality": 0.2}
            },
            {
                "name": "Havergal College", 
                "scores": {"academic": 4, "activities": 3, "culture": 4, "personality": 3},
                "weights": {"academic": 0.35, "activities": 0.25, "culture": 0.2, "personality": 0.2}
            }
        ]
    }
    
    conversation_log = [
        {"role": "student", "content": "æˆ‘å–œæ¬¢ç»„ç»‡æ´»åŠ¨å’Œåšç§‘å­¦å®éªŒ"},
        {"role": "parent", "content": "å­©å­åœ¨å­¦ç”Ÿä¼šç»„ç»‡è¿‡ç¯ä¿ä¹‰å–"}
    ]
    
    # ç”ŸæˆæŠ¥å‘Š
    report_result = generator_llm.generate_comprehensive_report(conversation_log, student_data)
    
    print("LLM PipelineæŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
    print(f"é¡µæ•°: {report_result['metadata']['page_count']}")
    print(f"å­—æ•°: {report_result['metadata']['word_count']}")
    
    # å¯¼å‡ºæŠ¥å‘Š
    exported_files = generator_llm.export_report(report_result, "all")
    
    print("\nLLM Pipelineå¯¼å‡ºæ–‡ä»¶:")
    for format_type, file_path in exported_files.items():
        print(f"{format_type}: {file_path}")
    
    # æµ‹è¯•ä¼ ç»Ÿpipeline
    print("\n=== æµ‹è¯•ä¼ ç»Ÿpipeline ===")
    generator_traditional = EnhancedReportGenerator(use_llm_pipeline=False)
    
    # ç”ŸæˆæŠ¥å‘Š
    report_result_traditional = generator_traditional.generate_comprehensive_report(conversation_log, student_data)
    
    print("ä¼ ç»ŸPipelineæŠ¥å‘Šç”ŸæˆæˆåŠŸ!")
    print(f"é¡µæ•°: {report_result_traditional['metadata']['page_count']}")
    print(f"å­—æ•°: {report_result_traditional['metadata']['word_count']}")
    
    # å¯¼å‡ºæŠ¥å‘Š
    exported_files_traditional = generator_traditional.export_report(report_result_traditional, "all")
    
    print("\nä¼ ç»ŸPipelineå¯¼å‡ºæ–‡ä»¶:")
    for format_type, file_path in exported_files_traditional.items():
        print(f"{format_type}: {file_path}")

if __name__ == "__main__":
    main()
