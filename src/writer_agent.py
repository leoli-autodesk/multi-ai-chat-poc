#!/usr/bin/env python3
"""
Writer Agent - ä¸“ä¸šæŠ¥å‘Šæ’°å†™ä»£ç†
å®ç°ç²¾ç¡®çš„Writer Agentå¥‘çº¦å’Œè°ƒç”¨æ–¹å¼
"""

import json
import re
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional
from cursor_ai import CursorAI

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WriterAgent:
    """Writer Agent - ä¸“ä¸šæŠ¥å‘Šæ’°å†™ä»£ç†"""
    
    def __init__(self, config_dir: str = "config"):
        """
        åˆå§‹åŒ–Writer Agent
        
        Args:
            config_dir: é…ç½®ç›®å½•è·¯å¾„
        """
        self.config_dir = Path(config_dir)
        self.logs_dir = Path("logs")
        self.logs_dir.mkdir(exist_ok=True)
        
        # åˆå§‹åŒ–AIæ¥å£ - ç¡®å®šæ€§è®¾ç½®
        self.ai = CursorAI({
            "globals": {"max_content_length": 300},
            "model_config": {
                "default_model": "gpt-4o",
                "temperature": 0.5,  # å›ºå®šæ¸©åº¦
                "top_p": 0.9,
                "max_tokens": 3500,
                "seed": 42  # å›ºå®šç§å­ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            }
        })
        
        # Writer Agentç³»ç»Ÿæç¤ºè¯ - ä¸¥æ ¼6ç« æ¨¡æ¿
        self.system_prompt = """ä½ æ˜¯ä¸€åèµ„æ·±ç§ç«‹å­¦æ ¡ç”³è¯·é¡¾é—®çš„ä¸“ä¸šæ’°ç¨¿äººï¼ˆWriterï¼‰ã€‚ä½ çš„å”¯ä¸€äº§å‡ºæ˜¯ä¸­æ–‡æ­£å¼ä¹¦é¢ä½“çš„è¿è´¯æ®µè½ï¼Œç”¨äºç›´æ¥æ¸²æŸ“åˆ° Wordã€‚

ã€ä¸¥æ ¼çº¦æŸã€‘
1. ä»…ç”Ÿæˆä»¥ä¸‹6ç« å†…å®¹ï¼Œä¸å¾—å¦èµ·ä»»ä½•é™„åŠ ç« èŠ‚æˆ–é‡å¤æ®µè½ï¼š
   - å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯
   - å­¦æ ¡ç”³è¯·å®šä½  
   - å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦
   - å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡
   - ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥
   - å½•å–åå»¶ä¼¸å»ºè®®

2. ä¸¥æ ¼ç¦æ­¢è¾“å‡ºä»»ä½• Markdown è¯­æ³•ï¼ˆå¦‚ #ã€**ã€[]()ã€-ã€|ã€è¡¨æ ¼ï¼‰ã€emojiã€å£è¯­åŒ–ã€æ„Ÿå¹å·ã€å ä½ç¬¦ï¼ˆå¦‚"ï¼ˆç”±é¢è°ˆè¡¥å……ï¼‰/TBD/TODO"ï¼‰å’Œçº¯åˆ—ç‚¹ã€‚

3. å¿…é¡»æŠŠè¾“å…¥çš„è¦ç‚¹ã€æ‰“åˆ†ã€æ¸…å•ã€å­å¼¹ç‚¹æ”¹å†™ä¸ºè‡ªç„¶æ®µè½ï¼Œå¥å¼æœ‰é•¿çŸ­å˜åŒ–ï¼Œé€»è¾‘æ¸…æ™°ï¼Œè¯æ®å……åˆ†ã€‚

4. åŒ¹é…åº¦ç« èŠ‚ä»…ä¸€æ¬¡ï¼ŒåŒ…å«é€æ ¡å·®å¼‚åŒ–çš„æ®µè½è§£è¯»ï¼Œç¦æ­¢åˆ—è¡¨ä¸è¡¨æ ¼ã€‚

5. åˆ†ç« è°ƒç”¨æ—¶ä¸å¾—å¤è¿°å‰æ–‡ï¼Œéœ€æ‰¿æ¥å¼ä¸€å¥å¸¦è¿‡å¹¶è¡¥å……æ–°ä¿¡æ¯ã€‚

ç›®æ ‡æˆå“æ˜¯ä¸€ä»½çº¦ 14â€“15 é¡µçš„ä¸“ä¸šæŠ¥å‘Šï¼Œå„ç« èŠ‚å­—æ•°èŒƒå›´å¦‚ä¸‹ï¼ˆä¸­æ–‡å­—ç¬¦ï¼Œä¸å«ç©ºæ ¼ï¼‰ï¼š

å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯ï¼š900â€“1100
å­¦æ ¡ç”³è¯·å®šä½ï¼š600â€“800
å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦ï¼ˆæ ¸å¿ƒï¼‰ï¼š1200â€“1500ï¼ˆå«é€æ ¡æ¨èç†ç”±æ¯æ ¡ 120â€“180ã€æ½œåœ¨æŒ‘æˆ˜ 80â€“120ï¼‰
å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡ï¼š900â€“1100
ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥ï¼š700â€“900
å½•å–åå»¶ä¼¸å»ºè®®ï¼š250â€“350

ç»“å°¾é¿å…å…ƒè¯è¯­ï¼ˆå¦‚"æœ¬ç« èŠ‚åˆ°æ­¤"ï¼‰ï¼Œå„æ®µä»¥å»ºè®®æˆ–è¡ŒåŠ¨è¦ç‚¹è‡ªç„¶æ”¶æŸã€‚
è‹¥è¾“å…¥ç¼ºé¡¹ï¼Œä¸å¾—è¾“å‡ºå ä½ç¬¦ï¼›åº”æ ¹æ®å¯å¾—ä¿¡æ¯ä¿å®ˆæ¨æ–­å¹¶æ ‡æ³¨"å®¶é•¿éœ€æ ¸å®ï¼šâ€¦"ï¼ˆå…¨ç¯‡â‰¤3å¤„ï¼‰ã€‚

ã€å»é‡ç¡¬çº¦æŸã€‘ï¼š
1. ä¸å¾—é‡å¤å·²åœ¨å‰æ–‡å†™è¿‡çš„è§‚ç‚¹æˆ–å¥å­ï¼›å½“éœ€è¦å†æ¬¡å¼•ç”¨æ—¶ï¼Œç”¨ 1 å¥æ‰¿æ¥å¼æ¦‚æ‹¬ï¼ˆä¾‹å¦‚"å‰æ–‡å·²è®ºåŠå…¶åœ¨ STEM çš„é•¿æœŸæŠ•å…¥ï¼Œæ­¤å¤„è¡¥å……å…¶åœ¨è·¨å­¦ç§‘å­¦ä¹ ä¸­çš„å»¶ä¼¸è¡¨ç°ã€‚"ï¼‰ï¼Œç¦æ­¢å¤è¿°åŸæ–‡ã€‚
2. é€æ ¡"æ¨èç†ç”±/æ½œåœ¨æŒ‘æˆ˜"å¿…é¡»å·®å¼‚åŒ–ï¼šæ¯æ‰€å­¦æ ¡è‡³å°‘åŒ…å« 2 ä¸ªä¸è¯¥æ ¡èµ„æº/æ–‡åŒ–/é¡¹ç›®å¼ºç›¸å…³çš„ç‹¬æœ‰è§’åº¦ï¼›ç¦æ­¢ä½¿ç”¨æ¨¡æ¿åŒ–å¥å¼ï¼ˆå¦‚"å­¦æœ¯å“è¶Šã€é¢†å¯¼åŠ›åŸ¹å…»ã€æ ¡å‹ç½‘ç»œå¼ºå¤§"ç­‰ï¼‰é‡å¤å‡ºç°ä¸¤æ¬¡ä»¥ä¸Šã€‚
3. ä»»ä½•ç« èŠ‚è‹¥ä¸å·²æœ‰å†…å®¹é«˜åº¦é‡å ï¼Œä¼˜å…ˆè¡¥å……æ–°çš„è¯æ®/ç¤ºä¾‹/å¯æ‰§è¡Œæ­¥éª¤ï¼Œè€Œéæ¢è¯åŒä¹‰æ”¹å†™ã€‚
4. ç¦æ­¢ä½¿ç”¨"æˆ‘ä»¬çš„ä¸“ä¸šä»·å€¼/æˆåŠŸä¿éšœ"ç±»è¥é”€æ®µè½è¶…è¿‡ 1 æ¬¡ï¼›è‹¥éœ€è¦ï¼Œè¯·åªä¿ç•™åœ¨æœ€å"æˆåŠŸå±•æœ›"å¤„ã€‚

ã€ç« èŠ‚å”¯ä¸€æ€§çº¦æŸã€‘ï¼š
5. ç¦æ­¢é‡å¤å·²å†™è¿‡çš„æ®µè½æˆ–ç« èŠ‚å†…å®¹ã€‚æ¯ä¸ªç« èŠ‚å†…å®¹åªå†™ä¸€æ¬¡ã€‚
6. è‹¥éœ€è¦æ‰¿æ¥å‰æ–‡ï¼Œè¯·ç”¨ä¸€å¥æ‰¿æ¥å¼æ€»ç»“ï¼Œè€Œä¸æ˜¯å¤è¿°ã€‚
7. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹6ä¸ªç« èŠ‚é¡ºåºç”Ÿæˆï¼Œæ¯ä¸ªç« èŠ‚åªå‡ºç°ä¸€æ¬¡ï¼š
   - å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯
   - å­¦æ ¡ç”³è¯·å®šä½  
   - å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦
   - å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡
   - ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥
   - å½•å–åå»¶ä¼¸å»ºè®®"""
        
        # ç« èŠ‚é…ç½®
        self.section_configs = {
            "å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯": {"min": 900, "max": 1100},
            "å­¦æ ¡ç”³è¯·å®šä½": {"min": 600, "max": 800},
            "å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦": {"min": 1200, "max": 1500},
            "å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡": {"min": 900, "max": 1100},
            "ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥": {"min": 700, "max": 900},
            "å½•å–åå»¶ä¼¸å»ºè®®": {"min": 250, "max": 350}
        }
    
    def write_section(self, section_name: str, section_json: Dict[str, Any], 
                     min_chars: int, max_chars: int, context_summary: str = "") -> str:
        """
        æ’°å†™å•ä¸ªç« èŠ‚
        
        Args:
            section_name: ç« èŠ‚åç§°
            section_json: ç« èŠ‚æ•°æ®
            min_chars: æœ€å°å­—æ•°
            max_chars: æœ€å¤§å­—æ•°
            context_summary: å·²å†™å†…å®¹æ‘˜è¦
            
        Returns:
            ç« èŠ‚å†…å®¹
        """
        # æ„å»ºç”¨æˆ·æç¤ºè¯
        user_prompt = f"""ç›®æ ‡ï¼šæ’°å†™ã€Šç§ç«‹å­¦æ ¡ç”³è¯·å’¨è¯¢æŠ¥å‘Šã€‹çš„ {section_name} ç« èŠ‚ï¼Œè¾“å‡ºè¿è´¯æ®µè½ã€‚
èµ„æ–™ï¼ˆJSONï¼‰ï¼š{json.dumps(section_json, ensure_ascii=False, indent=2)}
è§„åˆ™ï¼š

ç¦æ­¢åˆ—è¡¨/è¡¨æ ¼/emoji/Markdown/å ä½ç¬¦ï¼›åªå†™è‡ªç„¶æ®µã€‚

å­—æ•° {min_chars}â€“{max_chars}ï¼Œä¼˜å…ˆç»™å‡ºç»†èŠ‚ã€ä¾‹è¯ä¸å¯æ‰§è¡Œå»ºè®®ï¼›é¿å…ç©ºè¯ã€‚

ä¸ç¡®å®šä¿¡æ¯è¯·å®¡æ…è¡¨è¿°ï¼Œå¹¶é™„"å®¶é•¿éœ€æ ¸å®ï¼šâ€¦"ï¼ˆæœ¬ç« æœ€å¤š 1 å¤„ï¼‰ã€‚

åŒ¹é…åº¦ç« èŠ‚éœ€å¯¹æ¯æ‰€å­¦æ ¡å†™å‡ºç»¼åˆè§£è¯»ï¼ˆå­¦æœ¯/æ´»åŠ¨/æ–‡åŒ–/æ€§æ ¼å››ç»´ï¼‰ã€é‡åŒ–ç»“æœçš„æ–‡å­—åŒ–è§£é‡Šã€ç”³è¯·åˆ‡å…¥ç‚¹ä¸é¢è¯•æ€è·¯ï¼›é€æ ¡"æ¨èç†ç”±"120â€“180å­—ã€"æ½œåœ¨æŒ‘æˆ˜"80â€“120å­—ï¼Œä»¥æ®µè½è¡¨è¾¾ã€‚

ä¸å‡ºç°"æœ¬æŠ¥å‘Š/æœ¬ç« èŠ‚/å¦‚ä¸‹/ä»¥ä¸Š"ç­‰å…ƒè¯è¯­ï¼›ä»¥è¡ŒåŠ¨å»ºè®®æˆ–è§‚å¯Ÿè¦ç‚¹æ”¶æŸã€‚"""

        # å¦‚æœæœ‰å·²å†™å†…å®¹æ‘˜è¦ï¼Œæ·»åŠ å»é‡çº¦æŸ
        if context_summary:
            user_prompt += f"""

å·²å†™å†…å®¹æ‘˜è¦ï¼š{context_summary}

ä¸‹æ–‡æ’°å†™ä¸å¾—é‡å¤ context_summary çš„è¦ç‚¹ï¼›å¦‚éœ€æåŠï¼Œä»…ç”¨ä¸€å¥æ‰¿æ¥å¼æ¦‚æ‹¬ï¼Œå¹¶è¡¥å……æ–°çš„å¯æ ¸æŸ¥ç»†èŠ‚æˆ–æ­¥éª¤ã€‚"""

        # å¦‚æœæ˜¯åŒ¹é…åº¦ç« èŠ‚ï¼Œæ·»åŠ é¢å¤–è¦æ±‚
        if section_name == "å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦":
            user_prompt += """

ï¼ˆåŒ¹é…åº¦ç« é¢å¤–è¿½åŠ ï¼‰ï¼š

å°† matching.ranking çš„å…ˆåè½¬å†™ä¸º"é¡¾é—®æ¨èé¡ºåº"ï¼Œæ–‡å­—è§£é‡Šä¸æ­£æ–‡ä¸€è‡´ã€‚

å¦‚ facts ç¼ºå¤±ï¼Œè¯·ä¸è¾“å‡ºå ä½ç¬¦ï¼Œæ”¹ä¸ºç»™å‡ºä¿¡æ¯æ”¶é›†è·¯å¾„ï¼ˆå¦‚"æ ¡æ–¹è¯¾ç¨‹æ‰‹å†Œ/æ‹›ç”ŸåŠé‚®ä»¶æ ¸å®å¸ˆç”Ÿæ¯”"ï¼‰ã€‚"""
        
        try:
            # è°ƒè¯•ï¼šæ‰“å°æç¤ºè¯
            logger.info(f"ç”Ÿæˆç« èŠ‚ {section_name}ï¼Œå­—æ•°è¦æ±‚: {min_chars}-{max_chars}")
            logger.info(f"ç”¨æˆ·æç¤ºè¯é•¿åº¦: {len(user_prompt)}")
            
            # è°ƒç”¨AIç”Ÿæˆå†…å®¹
            response = self.ai.call_llm("Writer", self.system_prompt, {"content": user_prompt})
            
            if isinstance(response, str):
                logger.info(f"AIå“åº”é•¿åº¦: {len(response)}")
                
                # æ¸…ç†å†…å®¹
                cleaned_content = self.sanitize_to_prose(response)
                
                # ç¡®ä¿å†…å®¹ç¬¦åˆç« èŠ‚è¦æ±‚
                if len(cleaned_content) < min_chars:
                    logger.warning(f"ç« èŠ‚ {section_name} å­—æ•°ä¸è¶³ï¼Œéœ€è¦æ‰©å†™")
                    # å¯ä»¥åœ¨è¿™é‡Œå®ç°æ‰©å†™é€»è¾‘
                
                return cleaned_content
            else:
                logger.error(f"AIè¿”å›éå­—ç¬¦ä¸²å“åº”: {response}")
                return f"ï¼ˆå¾…å®¶é•¿ç¡®è®¤ï¼š{section_name}ç« èŠ‚å†…å®¹ï¼‰"
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆç« èŠ‚ {section_name} å¤±è´¥: {e}")
            return f"ï¼ˆå¾…å®¶é•¿ç¡®è®¤ï¼š{section_name}ç« èŠ‚å†…å®¹ï¼‰"
    
    def compose_full_report(self, data: Dict[str, Any]) -> str:
        """
        æ’°å†™å®Œæ•´æŠ¥å‘Š
        
        Args:
            data: å®Œæ•´æ•°æ®
            
        Returns:
            å®Œæ•´æŠ¥å‘Šå†…å®¹
        """
        logger.info("å¼€å§‹æ’°å†™å®Œæ•´æŠ¥å‘Š...")
        
        # æŒ‰å›ºå®šç« èŠ‚é¡ºåºç”Ÿæˆ
        section_order = [
            "å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯",
            "å­¦æ ¡ç”³è¯·å®šä½", 
            "å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦",
            "å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡",
            "ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥",
            "å½•å–åå»¶ä¼¸å»ºè®®"
        ]
        
        sections_content = {}
        context_summary = ""  # å·²å†™å†…å®¹æ‘˜è¦
        
        # æŒ‰ç« èŠ‚é¡ºåºç”Ÿæˆ
        for section_name in section_order:
            logger.info(f"æ­£åœ¨æ’°å†™ç« èŠ‚: {section_name}")
            
            # æå–ç« èŠ‚æ•°æ®
            section_data = self.extract_section_data(data, section_name)
            
            # ç”Ÿæˆç« èŠ‚å†…å®¹ï¼ˆä¼ å…¥å·²å†™å†…å®¹æ‘˜è¦ï¼‰
            section_content = self.write_section(
                section_name, 
                section_data, 
                self.section_configs[section_name]["min"], 
                self.section_configs[section_name]["max"],
                context_summary
            )
            
            # å­˜å‚¨ç« èŠ‚å†…å®¹ï¼ˆä¸åŒ…å«æ ‡é¢˜ï¼‰
            sections_content[section_name] = section_content
            
            # æ›´æ–°å·²å†™å†…å®¹æ‘˜è¦ï¼ˆå–å‰3-5å¥ï¼‰
            section_sentences = self.extract_key_sentences(section_content)
            if section_sentences:
                context_summary += f"{section_name}è¦ç‚¹ï¼š{' '.join(section_sentences[:3])}ã€‚"
        
        # æŒ‰æ¨¡æ¿é¡ºåºæ‹¼æ¥ç« èŠ‚
        full_report = self.build_report_by_template(sections_content)
        
        # å»é‡ç« èŠ‚
        deduplicated_report = self.deduplicate_sections(full_report)
        
        logger.info("å®Œæ•´æŠ¥å‘Šæ’°å†™å®Œæˆ")
        return deduplicated_report
    
    def generate_section(self, prompt: str) -> str:
        """ç”Ÿæˆå•ä¸ªç« èŠ‚å†…å®¹"""
        try:
            # è°ƒç”¨AIç”Ÿæˆå†…å®¹
            response = self.ai.generate_writer_response(prompt)
            
            # æ¸…ç†å†…å®¹
            cleaned_content = self.clean_content(response)
            
            return cleaned_content
            
        except Exception as e:
            logger.error(f"ç« èŠ‚ç”Ÿæˆå¤±è´¥: {e}")
            return f"ç« èŠ‚å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}"
    
    def clean_content(self, content: str) -> str:
        """æ¸…ç†å†…å®¹ï¼Œç§»é™¤Markdownå’Œemoji"""
        # ç§»é™¤Markdownæ ‡è®°
        content = re.sub(r'[#*`\[\]()|]', '', content)
        
        # ç§»é™¤emoji
        emoji_pattern = r'[âœ…ğŸ¯ğŸ“…ğŸ‰ğŸ“ŠğŸ“‹ğŸ†ğŸ’¡ğŸš€â­]'
        content = re.sub(emoji_pattern, '', content)
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        content = re.sub(r'\s+', ' ', content)
        
        return content.strip()
    
    def extract_section_data(self, data: Dict[str, Any], section_name: str) -> Dict[str, Any]:
        """æå–ç« èŠ‚ç›¸å…³æ•°æ®"""
        if section_name == "å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯":
            return {
                "student": data.get("student", {}),
                "family": data.get("family", {})
            }
        elif section_name == "å­¦æ ¡ç”³è¯·å®šä½":
            return {
                "positioning": data.get("positioning", {}),
                "family": data.get("family", {})
            }
        elif section_name == "å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦":
            return {
                "matching": data.get("matching", {}),
                "student": data.get("student", {}),
                "family": data.get("family", {})
            }
        elif section_name == "å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡":
            return {
                "plans": data.get("plans", {}),
                "student": data.get("student", {})
            }
        elif section_name == "ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥":
            return {
                "timeline": data.get("timeline", {}),
                "tests": data.get("tests", {}),
                "essays_refs_interview": data.get("essays_refs_interview", {}),
                "student": data.get("student", {})
            }
        elif section_name == "å½•å–åå»¶ä¼¸å»ºè®®":
            return {
                "post_offer": data.get("post_offer", {}),
                "student": data.get("student", {})
            }
        else:
            return {}
    
    def sanitize_to_prose(self, content: str) -> str:
        """
        æ–‡æœ¬æ¸…æ´—ï¼ˆå¼ºåˆ¶ï¼‰
        åˆ é™¤æ‰€æœ‰å¯èƒ½æ®‹ç•™çš„ï¼šMarkdown è¯­æ³•ã€emojiã€éæ ‡å‡†ç©ºç™½ç¬¦
        """
        # åˆ é™¤Markdownè¯­æ³•
        markdown_patterns = [
            r'\*\*(.*?)\*\*',  # ç²—ä½“
            r'\*(.*?)\*',      # æ–œä½“
            r'#+\s*',          # æ ‡é¢˜
            r'^\s*[-*+]\s*',   # åˆ—è¡¨
            r'^\s*\d+\.\s*',   # æ•°å­—åˆ—è¡¨
            r'\|.*?\|',        # è¡¨æ ¼
            r'```.*?```',      # ä»£ç å—
            r'\[.*?\]\(.*?\)', # é“¾æ¥
            r'`.*?`',          # è¡Œå†…ä»£ç 
        ]
        
        for pattern in markdown_patterns:
            content = re.sub(pattern, '', content, flags=re.MULTILINE)
        
        # åˆ é™¤emoji
        emoji_pattern = re.compile("["
            u"\U0001F600-\U0001F64F"  # emoticons
            u"\U0001F300-\U0001F5FF"  # symbols & pictographs
            u"\U0001F680-\U0001F6FF"  # transport & map symbols
            u"\U0001F1E0-\U0001F1FF"  # flags (iOS)
            u"\U00002600-\U000026FF"  # miscellaneous symbols
            u"\U00002700-\U000027BF"  # dingbats
            "]+", flags=re.UNICODE)
        content = emoji_pattern.sub('', content)
        
        # åˆ é™¤å ä½ç¬¦
        placeholder_patterns = [
            r'ï¼ˆç”±é¢è°ˆè¡¥å……ï¼‰',
            r'ï¼ˆTBDï¼‰',
            r'ï¼ˆTODOï¼‰',
            r'/\*\*.*?\*\*/',  # æ³¨é‡Š
        ]
        
        for pattern in placeholder_patterns:
            content = re.sub(pattern, '', content)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œå’Œç©ºç™½ç¬¦
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = re.sub(r'[ \t]+', ' ', content)
        
        # åˆå¹¶è¿‡çŸ­çš„å¥å­
        content = self.merge_short_sentences(content)
        
        return content.strip()
    
    def merge_short_sentences(self, content: str) -> str:
        """åˆå¹¶è¿‡çŸ­çš„å¥å­ä¸º3-6å¥è‡ªç„¶æ®µ"""
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        paragraphs = []
        current_paragraph = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            current_paragraph.append(sentence)
            
            # å¦‚æœå½“å‰æ®µè½æœ‰3-6å¥ï¼Œå¼€å§‹æ–°æ®µè½
            if len(current_paragraph) >= 3:
                paragraphs.append('ã€‚'.join(current_paragraph) + 'ã€‚')
                current_paragraph = []
        
        # å¤„ç†å‰©ä½™çš„å¥å­
        if current_paragraph:
            paragraphs.append('ã€‚'.join(current_paragraph) + 'ã€‚')
        
        return '\n\n'.join(paragraphs)
    
    def validate_content(self, content: str) -> Dict[str, Any]:
        """éªŒè¯å†…å®¹è´¨é‡"""
        validation_result = {
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "word_count": len(content),
            "has_markdown": bool(re.search(r'[*#|`]', content)),
            "has_emoji": bool(re.search(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF]', content)),
            "has_placeholders": bool(re.search(r'ï¼ˆç”±é¢è°ˆè¡¥å……ï¼‰|ï¼ˆTBDï¼‰|ï¼ˆTODOï¼‰', content)),
            "sections_found": len(re.findall(r'å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯|å­¦æ ¡ç”³è¯·å®šä½|å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦|å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡|ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥|å½•å–åå»¶ä¼¸å»ºè®®', content)),
            "needs_rewrite": False,
            "rewrite_reasons": []
        }
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å†™
        if validation_result["has_markdown"]:
            validation_result["needs_rewrite"] = True
            validation_result["rewrite_reasons"].append("åŒ…å«Markdownè¯­æ³•")
        
        if validation_result["has_emoji"]:
            validation_result["needs_rewrite"] = True
            validation_result["rewrite_reasons"].append("åŒ…å«emoji")
        
        if validation_result["has_placeholders"]:
            validation_result["needs_rewrite"] = True
            validation_result["rewrite_reasons"].append("åŒ…å«å ä½ç¬¦")
        
        if validation_result["sections_found"] < 6:
            validation_result["needs_rewrite"] = True
            validation_result["rewrite_reasons"].append("ç« èŠ‚æ•°é‡ä¸è¶³")
        
        return validation_result
    
    def log_writer_summary(self, validation_result: Dict[str, Any], 
                          section_counts: Dict[str, int]) -> None:
        """è®°å½•Writeræ‘˜è¦æ—¥å¿—"""
        try:
            summary = {
                "timestamp": validation_result["timestamp"],
                "word_count": validation_result["word_count"],
                "sections_found": validation_result["sections_found"],
                "section_word_counts": section_counts,
                "needs_rewrite": validation_result["needs_rewrite"],
                "rewrite_reasons": validation_result["rewrite_reasons"],
                "has_markdown": validation_result["has_markdown"],
                "has_emoji": validation_result["has_emoji"],
                "has_placeholders": validation_result["has_placeholders"]
            }
            
            log_file = self.logs_dir / "writer_summary.json"
            
            # è¯»å–ç°æœ‰æ—¥å¿—
            if log_file.exists():
                with open(log_file, 'r', encoding='utf-8') as f:
                    logs = json.load(f)
            else:
                logs = []
            
            # æ·»åŠ æ–°è®°å½•
            logs.append(summary)
            
            # åªä¿ç•™æœ€è¿‘50æ¡è®°å½•
            if len(logs) > 50:
                logs = logs[-50:]
            
            # å†™å…¥æ—¥å¿—
            with open(log_file, 'w', encoding='utf-8') as f:
                json.dump(logs, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            logger.error(f"è®°å½•Writeræ‘˜è¦å¤±è´¥: {e}")
    
    def count_section_words(self, content: str) -> Dict[str, int]:
        """ç»Ÿè®¡å„ç« èŠ‚å­—æ•°"""
        section_counts = {}
        
        # æŒ‰ç« èŠ‚åˆ†å‰²å†…å®¹
        sections = self.split_content_by_sections(content)
        
        for section_name, section_content in sections.items():
            section_counts[section_name] = len(section_content)
        
        return section_counts
    
    def split_content_by_sections(self, content: str) -> Dict[str, str]:
        """æŒ‰ç« èŠ‚åˆ†å‰²å†…å®¹"""
        sections = {}
        
        section_patterns = {
            "å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯": r"å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯",
            "å­¦æ ¡ç”³è¯·å®šä½": r"å­¦æ ¡ç”³è¯·å®šä½",
            "å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦": r"å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦",
            "å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡": r"å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡",
            "ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥": r"ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥",
            "å½•å–åå»¶ä¼¸å»ºè®®": r"å½•å–åå»¶ä¼¸å»ºè®®"
        }
        
        lines = content.split('\n')
        current_section = None
        current_content = []
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
                
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
            for section_name, pattern in section_patterns.items():
                if re.search(pattern, line):
                    # ä¿å­˜å‰ä¸€ç« èŠ‚
                    if current_section and current_content:
                        sections[current_section] = '\n'.join(current_content)
                    
                    # å¼€å§‹æ–°ç« èŠ‚
                    current_section = section_name
                    current_content = []
                    break
            else:
                # æ·»åŠ åˆ°å½“å‰ç« èŠ‚
                if current_section:
                    current_content.append(line)
        
        # ä¿å­˜æœ€åä¸€ç« èŠ‚
        if current_section and current_content:
            sections[current_section] = '\n'.join(current_content)
        
        return sections
    
    def extract_key_sentences(self, content: str) -> List[str]:
        """æå–å…³é”®å¥å­ï¼ˆå‰3-5å¥ï¼‰"""
        # æŒ‰å¥å·åˆ†å‰²å¥å­
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', content)
        
        # è¿‡æ»¤ç©ºå¥å­å’Œè¿‡çŸ­çš„å¥å­
        key_sentences = []
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) > 10:  # è‡³å°‘10ä¸ªå­—ç¬¦
                key_sentences.append(sentence)
                if len(key_sentences) >= 5:  # æœ€å¤šå–5å¥
                    break
        
        return key_sentences
    
    def build_report_by_template(self, sections_content: Dict[str, str]) -> str:
        """
        æŒ‰æ¨¡æ¿é¡ºåºæ‹¼æ¥ç« èŠ‚
        
        Args:
            sections_content: ç« èŠ‚å†…å®¹å­—å…¸
            
        Returns:
            æ‹¼æ¥åçš„æŠ¥å‘Š
        """
        section_order = [
            "å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯",
            "å­¦æ ¡ç”³è¯·å®šä½", 
            "å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦",
            "å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡",
            "ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥",
            "å½•å–åå»¶ä¼¸å»ºè®®"
        ]
        
        report_parts = []
        
        for section_name in section_order:
            if section_name in sections_content:
                # æ·»åŠ ç« èŠ‚æ ‡é¢˜
                report_parts.append(section_name)
                # æ·»åŠ ç« èŠ‚å†…å®¹
                report_parts.append(sections_content[section_name])
                # æ·»åŠ ç©ºè¡Œåˆ†éš”
                report_parts.append("")
        
        return "\n".join(report_parts)
    
    def deduplicate_sections(self, text: str) -> str:
        """
        æŒ‰æ ‡é¢˜é”šç‚¹æ£€æµ‹ï¼Œåªä¿ç•™é¦–æ¬¡å‡ºç°çš„å†…å®¹ï¼Œåç»­ç›¸åŒæ ‡é¢˜æ®µè½å…¨éƒ¨ä¸¢å¼ƒ
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            å»é‡åçš„æ–‡æœ¬
        """
        logger.info("å¼€å§‹å»é‡ç« èŠ‚...")
        
        # å®šä¹‰ç« èŠ‚æ ‡é¢˜é”šç‚¹
        section_anchors = [
            "å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯",
            "å­¦æ ¡ç”³è¯·å®šä½", 
            "å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦",
            "å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡",
            "ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥",
            "å½•å–åå»¶ä¼¸å»ºè®®"
        ]
        
        lines = text.split('\n')
        deduplicated_lines = []
        seen_sections = set()
        current_section = None
        in_section = False
        
        for line in lines:
            line = line.strip()
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç« èŠ‚æ ‡é¢˜
            is_section_title = False
            for anchor in section_anchors:
                if line == anchor:
                    is_section_title = True
                    break
            
            if is_section_title:
                if line not in seen_sections:
                    # é¦–æ¬¡å‡ºç°çš„ç« èŠ‚ï¼Œä¿ç•™
                    seen_sections.add(line)
                    current_section = line
                    in_section = True
                    deduplicated_lines.append(line)
                    logger.info(f"ä¿ç•™ç« èŠ‚: {line}")
                else:
                    # é‡å¤ç« èŠ‚ï¼Œä¸¢å¼ƒ
                    current_section = None
                    in_section = False
                    logger.info(f"ä¸¢å¼ƒé‡å¤ç« èŠ‚: {line}")
            else:
                # æ™®é€šå†…å®¹è¡Œ
                if in_section:
                    # åœ¨æœ‰æ•ˆç« èŠ‚å†…ï¼Œä¿ç•™å†…å®¹
                    deduplicated_lines.append(line)
                elif not line:
                    # ç©ºè¡Œï¼Œä¿ç•™
                    deduplicated_lines.append(line)
                # å…¶ä»–æƒ…å†µï¼ˆåœ¨é‡å¤ç« èŠ‚å†…çš„å†…å®¹ï¼‰ä¸¢å¼ƒ
        
        result = '\n'.join(deduplicated_lines)
        
        # æ¸…ç†å¤šä½™çš„ç©ºè¡Œ
        result = re.sub(r'\n\s*\n\s*\n', '\n\n', result)
        
        logger.info(f"ç« èŠ‚å»é‡å®Œæˆï¼Œä¿ç•™ç« èŠ‚æ•°: {len(seen_sections)}")
        return result.strip()
    
    def validate_section_count(self, text: str) -> Dict[str, Any]:
        """
        éªŒè¯ç« èŠ‚æ•°é‡å’Œè´¨é‡
        
        Args:
            text: æŠ¥å‘Šæ–‡æœ¬
            
        Returns:
            éªŒè¯ç»“æœ
        """
        section_anchors = [
            "å®¶åº­ä¸å­¦ç”ŸèƒŒæ™¯",
            "å­¦æ ¡ç”³è¯·å®šä½", 
            "å­¦ç”Ÿâ€”å­¦æ ¡åŒ¹é…åº¦",
            "å­¦æœ¯ä¸è¯¾å¤–å‡†å¤‡",
            "ç”³è¯·æµç¨‹ä¸ä¸ªæ€§åŒ–ç­–ç•¥",
            "å½•å–åå»¶ä¼¸å»ºè®®"
        ]
        
        found_sections = []
        for anchor in section_anchors:
            if anchor in text:
                found_sections.append(anchor)
        
        validation_result = {
            "total_sections": len(found_sections),
            "expected_sections": 6,
            "found_sections": found_sections,
            "missing_sections": [s for s in section_anchors if s not in found_sections],
            "is_valid": len(found_sections) == 6,
            "has_duplicates": len(found_sections) != len(set(found_sections))
        }
        
        return validation_result


def main():
    """æµ‹è¯•Writer Agent"""
    writer = WriterAgent()
    
    # æµ‹è¯•æ•°æ®
    test_data = {
        "student": {
            "name": "Alex Chen",
            "age": "14å²",
            "grade": "Grade 8",
            "gpa": "3.8/4.0",
            "academic_strengths": "æ•°å­¦ã€ç‰©ç†ã€è®¡ç®—æœºç§‘å­¦",
            "competition_achievements": "æœºå™¨äººç«èµ›çœçº§äºŒç­‰å¥–",
            "leadership_positions": "ç§‘æŠ€éƒ¨å‰¯éƒ¨é•¿",
            "project_experiences": "ç¯ä¿ä¹‰å–æ´»åŠ¨ç»„ç»‡"
        },
        "family": {
            "education_values": "é‡è§†å…¨äººæ•™è‚²ï¼ŒåŸ¹å…»ç‹¬ç«‹æ€è€ƒå’Œåˆ›æ–°èƒ½åŠ›",
            "goals": "å¸Œæœ›å­©å­åœ¨å›½é™…åŒ–ç¯å¢ƒä¸­å…¨é¢å‘å±•",
            "culture": "ä¸­è¥¿æ–‡åŒ–èåˆï¼Œé‡è§†ä¼ ç»Ÿä»·å€¼è§‚",
            "support_level": "å…¨åŠ›æ”¯æŒå­©å­çš„æ•™è‚²å’Œå‘å±•"
        },
        "positioning": {
            "parent_criteria": ["å­¦æœ¯", "å…¨äººæ•™è‚²", "ä½“è‚²", "è‰ºæœ¯"],
            "school_type_preference": "ç§ç«‹å­¦æ ¡",
            "location_preference": "å¤šä¼¦å¤šåœ°åŒº",
            "budget_range": "ä¸­ç­‰åä¸Š"
        },
        "matching": {
            "weights": {"academic": 0.35, "activities": 0.25, "culture": 0.2, "personality": 0.2},
            "schools": [
                {
                    "name": "Upper Canada College",
                    "facts": {"ratio": "8:1", "location": "Toronto", "tuition": 55000, "class_size": "20â€“25"},
                    "scores": {"academic": 5, "activities": 5, "culture": 5, "personality": 5},
                    "match_percentage": 92,
                    "advantages": ["STEMé¡¹ç›®ä¸°å¯Œ", "å­¦æœ¯ç¯å¢ƒä¼˜ç§€"],
                    "challenges": ["ç«äº‰æ¿€çƒˆ"],
                    "strategies": ["çªå‡ºé¢†å¯¼åŠ›", "å±•ç°STEMä¸“é•¿"]
                }
            ],
            "ranking": [
                {"name": "UCC", "reason": "å­¦æœ¯å’ŒSTEMé¡¹ç›®åŒ¹é…åº¦æœ€é«˜"},
                {"name": "Havergal", "reason": "å…¨äººæ•™è‚²ç†å¿µå¥‘åˆ"},
                {"name": "SAC", "reason": "ä¼ ç»Ÿä»·å€¼è§‚ä¸å®¶åº­èƒŒæ™¯åŒ¹é…"}
            ]
        },
        "plans": {
            "academic_preparation": "åŠ å¼ºè‹±è¯­å†™ä½œèƒ½åŠ›",
            "extracurricular_preparation": "å‚ä¸æ›´å¤šSTEMç«èµ›",
            "test_preparation": "SSATç›®æ ‡90th percentileä»¥ä¸Š"
        },
        "timeline": {
            "deadlines": ["10æœˆå®ŒæˆSSAT", "11æœˆæäº¤ç”³è¯·", "12æœˆå‚åŠ é¢è¯•"],
            "milestones": ["ææ–™å‡†å¤‡", "é¢è¯•å‡†å¤‡", "æœ€ç»ˆæäº¤"]
        },
        "tests": {
            "ssat": "ç›®æ ‡90th percentileä»¥ä¸Š",
            "isee": "å¤‡é€‰æ–¹æ¡ˆ",
            "school_tests": "å„æ ¡ç‰¹è‰²æµ‹è¯•"
        },
        "essays_refs_interview": {
            "essay_themes": ["é¢†å¯¼åŠ›ç»å†", "STEMå…´è¶£", "ç¤¾åŒºæœåŠ¡"],
            "recommendation_strategy": "æ¥è‡ªæŒ‡å¯¼è€å¸ˆå’Œç¤¾å›¢è´Ÿè´£äºº",
            "interview_preparation": "çªå‡ºç¯ä¿ä¹‰å–æ´»åŠ¨ç»éªŒ"
        },
        "post_offer": {
            "transition_preparation": "æå‰äº†è§£å­¦æ ¡æ–‡åŒ–",
            "academic_planning": "å‡†å¤‡å­¦æœ¯è¡”æ¥",
            "social_networking": "å»ºç«‹ç¤¾äº¤ç½‘ç»œ"
        }
    }
    
    # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
    print("æ­£åœ¨ç”Ÿæˆå®Œæ•´æŠ¥å‘Š...")
    full_report = writer.compose_full_report(test_data)
    
    print("æŠ¥å‘Šç”Ÿæˆå®Œæˆ!")
    print(f"æ€»å­—æ•°: {len(full_report)}")
    
    # éªŒè¯å†…å®¹
    validation_result = writer.validate_content(full_report)
    print(f"éªŒè¯ç»“æœ: {validation_result}")
    
    # ç»Ÿè®¡ç« èŠ‚å­—æ•°
    section_counts = writer.count_section_words(full_report)
    print(f"ç« èŠ‚å­—æ•°: {section_counts}")
    
    # è®°å½•æ—¥å¿—
    writer.log_writer_summary(validation_result, section_counts)
    
    # ä¿å­˜æŠ¥å‘Š
    output_file = Path("output") / f"writer_agent_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
    output_file.parent.mkdir(exist_ok=True)
    
    with open(output_file, 'w', encoding='utf-8') as f:
        f.write(full_report)
    
    print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")


if __name__ == "__main__":
    main()
